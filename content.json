{"pages":[{"title":"About Me","text":"无名之辈","link":"/about/index.html"},{"title":"","text":"{\"files\":{\"main.js\":\"/static/js/main.95e6d511.chunk.js\",\"main.js.map\":\"/static/js/main.95e6d511.chunk.js.map\",\"runtime-main.js\":\"/static/js/runtime-main.41da8003.js\",\"runtime-main.js.map\":\"/static/js/runtime-main.41da8003.js.map\",\"static/js/2.9a38754e.chunk.js\":\"/static/js/2.9a38754e.chunk.js\",\"static/js/2.9a38754e.chunk.js.map\":\"/static/js/2.9a38754e.chunk.js.map\",\"index.html\":\"/index.html\",\"static/js/2.9a38754e.chunk.js.LICENSE.txt\":\"/static/js/2.9a38754e.chunk.js.LICENSE.txt\"},\"entrypoints\":[\"static/js/runtime-main.41da8003.js\",\"static/js/2.9a38754e.chunk.js\",\"static/js/main.95e6d511.chunk.js\"]}","link":"/navi/asset-manifest.json"},{"title":"","text":"React AppYou need to enable JavaScript to run this app.!function(e){function r(r){for(var n,l,c=r[0],f=r[1],i=r[2],p=0,s=[];p","link":"/navi/index.html"},{"title":"","text":"{\"sites\":[{\"url\":\"https://aplus.huya.com/#/home/buildProject/index\",\"title\":\"构建系统\"},{\"url\":\"https://ermao.live/\",\"title\":\"我的博客\"},{\"url\":\"https://github.com/\",\"title\":\"github\"},{\"url\":\"https://stackoverflow.com/\",\"title\":\"stackoverflow\"},{\"url\":\"https://en.cppreference.com/w/\",\"title\":\"cppreference\"},{\"url\":\"https://www.ffmpeg.org/documentation.html\",\"title\":\"FFmpeg文档\"},{\"url\":\"https://tech.sina.com.cn/\",\"title\":\"新浪科技\"},{\"url\":\"https://maimai.cn/gossip_list\",\"title\":\"脉脉\"},{\"url\":\"https://www.zhihu.com/\",\"title\":\"知乎\"},{\"url\":\"https://weibo.com/liangzuoting/home?wvr=5\",\"title\":\"微博\"},{\"url\":\"https://v.qq.com/\",\"title\":\"腾讯视频\"},{\"url\":\"https://www.youku.com/\",\"title\":\"优酷\"},{\"url\":\"https://www.iqiyi.com/\",\"title\":\"爱奇艺\"},{\"url\":\"http://www.qsptv.net/\",\"title\":\"全视频\"},{\"url\":\"https://www.douyu.com/62081\",\"title\":\"叶落直播间\"},{\"url\":\"https://www.huya.com/11352915\",\"title\":\"老白涮肉坊\"},{\"url\":\"https://www.tmall.com/\",\"title\":\"天猫\"},{\"url\":\"https://www.taobao.com/\",\"title\":\"淘宝\"},{\"url\":\"https://www.ctrip.com/\",\"title\":\"携程\"}]}","link":"/navi/oftenuse.json"},{"title":"","text":"{\"categories\":[{\"name\":\"公司\",\"sites\":[{\"title\":\"OA\",\"url\":\"https://oa.huya.com/index.html#/mains\"},{\"title\":\"日志后台\",\"url\":\"https://ffilelogweb.huya.com/indexpage.html\"},{\"title\":\"YY UID查询\",\"url\":\"http://webdb.yyembed.yy.com/webdb\"},{\"title\":\"jenkins构建\",\"url\":\"https://jenkins.huya.com/\"},{\"title\":\"看板-主播基础平台组-TAPD平台\",\"url\":\"https://www.tapd.cn/39551029/board/index?board_id=1139551029001000094&board_type=standard&view_type=standard-board\"},{\"title\":\"sre业务支持系统\",\"url\":\"https://sre.huya.com/\"},{\"title\":\"游戏直播管理后台\",\"url\":\"http://admin.huya.com/login/\"}]},{\"name\":\"C++\",\"sites\":[{\"title\":\"cppreference\",\"url\":\"https://en.cppreference.com/w/\"},{\"title\":\"C++ Core Guidelines\",\"url\":\"https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md\"},{\"title\":\"coliru在线编译\",\"url\":\"http://coliru.stacked-crooked.com/\"},{\"title\":\"7 Features of C++17 that will simplify your code\",\"url\":\"https://www.codingame.com/playgrounds/2205/7-features-of-c17-that-will-simplify-your-code/introduction\"}]},{\"name\":\"Unity\",\"sites\":[{\"title\":\"2018.4文档\",\"url\":\"https://docs.unity3d.com/2018.4/Documentation/Manual/\"},{\"title\":\"UGUI基础！界面拼接！\",\"url\":\"https://zhuanlan.zhihu.com/p/28905447\"},{\"title\":\"Unity - Manual: Execution Order of Event Functions\",\"url\":\"https://docs.unity3d.com/2018.2/Documentation/Manual/ExecutionOrder.html\"},{\"title\":\"Unity - Manual: Getting started with Android development\",\"url\":\"https://docs.unity3d.com/Manual/android-GettingStarted.html\"},{\"title\":\"Unity教程 用户界面（UI）_w3cschool\",\"url\":\"https://www.w3cschool.cn/unity3d_jc/unity3d_jc-8lea2dkb.html\"},{\"title\":\"Unity 2018.4 中文手册 中文文档\",\"url\":\"https://unity2018.docs.zlogn.com/\"},{\"title\":\"Unity Play Video On Canvas | Learn Unity Mir Imad Ahmed\",\"url\":\"https://www.mirimad.com/unity-play-video-on-canvas/\"},{\"title\":\"Video - Change/Detect video orientation\",\"url\":\"https://forum.unity.com/threads/change-detect-video-orientation.546450/\"},{\"title\":\"360 VIDEO PLAYER (with VR mode) - Unity Tutorial - YouTube\",\"url\":\"https://www.youtube.com/watch?v=RxlQnPcOoYc\"}]},{\"name\":\"Android\",\"sites\":[{\"url\":\"https://my.oschina.net/lifj/blog/709188\",\"title\":\"［直播一揽子］x264 for Android 的编译\"},{\"url\":\"https://blog.csdn.net/chinabinlang/article/details/46898961\",\"title\":\"X264的ARMV7-a的交叉编译及优化运行\"},{\"url\":\"https://blog.csdn.net/xyang81/article/details/42319789\",\"title\":\"Android NDK开发Crash错误定位\"},{\"url\":\"https://www.jianshu.com/p/73fed068a795\",\"title\":\"Android Camera2 使用总结\"},{\"url\":\"https://www.jianshu.com/p/d83161e77e90\",\"title\":\"Android进阶——你所知道的Camera2和你所不知道的Camera2完全解析\"},{\"url\":\"https://source.android.com/devices/graphics/arch-egl-opengl\",\"title\":\"EGLSurface 和 OpenGL ES | Android Open Source Project\"},{\"url\":\"https://www.jianshu.com/p/c085af435d88\",\"title\":\"「音视频直播技术」Android下视频H264编码\"},{\"url\":\"https://juejin.im/post/5aefdb2c51882522835e6542\",\"title\":\"《OpenGL ES 2.0 for Android》读书笔记\"},{\"url\":\"https://www.khronos.org/registry/EGL/\",\"title\":\"Khronos EGL Registry - The Khronos Group Inc\"},{\"url\":\"https://developer.android.com/training/graphics/opengl/environment#java\",\"title\":\"Build an OpenGL ES environment | Android Developers\"},{\"url\":\"https://source.android.com/devices/graphics/architecture.html\",\"title\":\"图形架构 | Android Open Source Project\"},{\"url\":\"https://blog.csdn.net/cmsbupt/article/details/9815685\",\"title\":\"Android系统中YUV420p (NV21)到ARGB8888的转换\"},{\"url\":\"https://mp.weixin.qq.com/s/0xek4nnc2zoKpslEyILQmw?comefrom=https://blogread.cn/news/\",\"title\":\"总是听到有人说AndroidX，到底什么是AndroidX？\"},{\"url\":\"https://developer.android.com/guide/topics/media/camera\",\"title\":\"Camera API | Android Developers\"},{\"url\":\"https://developer.android.com/ndk/guides/audio/opensl/opensl-prog-notes.html#kotlin\",\"title\":\"OpenSL ES 编程说明 | Android NDK | Android Developers\"},{\"url\":\"https://developer.android.com/ndk/guides/audio/opensl/opensl-for-android\",\"title\":\"面向 Android 的 OpenSL ES | Android NDK | Android Developers\"},{\"url\":\"https://developer.android.com/ndk/guides/stable_apis\",\"title\":\"Android NDK 原生 API | Android Developers\"},{\"url\":\"http://www.soomal.com/doc/10100006276.htm\",\"title\":\"Gabor、Patrick作品 - Android的10毫秒问题 解读Android系统音频通道延迟缺陷[Soomal]\"},{\"url\":\"https://developer.android.com/ndk/guides/audio/audio-latency?hl=zh-cn\",\"title\":\"音频延迟 | Android NDK | Android Developers\"}]},{\"name\":\"TCP/IP\",\"sites\":[{\"url\":\"https://gist.github.com/gabrielfalcao/4216897\",\"title\":\"TCP socket error codes\"},{\"url\":\"https://blog.csdn.net/tianlongtc/article/details/80238497\",\"title\":\"TCP快速重传为什么是三次冗余ack\"},{\"url\":\"https://www.zhihu.com/question/21789252\",\"title\":\"TCP快速重传为什么是三次冗余ack，这个三次是怎么定下来的？\"},{\"url\":\"https://www.cnblogs.com/wetest/p/9190786.html\",\"title\":\"可靠UDP，KCP协议快在哪？\"},{\"url\":\"https://blog.csdn.net/qq_36748278/article/details/80171575\",\"title\":\"KCP原理及源码解析\"},{\"url\":\"https://chromium.googlesource.com/chromium/src/+/master/docs/windows_build_instructions.md\",\"title\":\"Checking out and Building Chromium for Windows\"},{\"url\":\"http://jagt.github.io/clumsy/cn/manual.html\",\"title\":\"clumsy, 在 Windows Vista / Windows 7 下模拟劣化网络环境(网络延迟，掉包，重发)的小工具。\"},{\"url\":\"https://zhuanlan.zhihu.com/p/32553477\",\"title\":\"科普：QUIC协议原理分析\"}]},{\"name\":\"iOS\",\"sites\":[{\"url\":\"https://www.jianshu.com/p/eccdcf43d7d2\",\"title\":\"iOS视频开发（一）：视频采集\"},{\"url\":\"https://blog.csdn.net/lincsdnnet/article/details/78255773\",\"title\":\"iOS - 视频采集详解\"},{\"url\":\"https://www.jianshu.com/p/9809760654ea\",\"title\":\"iOS-镜头采集(AVCaptureDevice)\"},{\"url\":\"https://www.jianshu.com/p/019f59a37e1e\",\"title\":\"iOS-镜头采集(Camera capture&amp;AVCaptureSession)\"},{\"url\":\"https://www.jianshu.com/p/d99366dd19e4\",\"title\":\"iOS 相机捕捉\"},{\"url\":\"https://www.jianshu.com/p/8b28892bae5a\",\"title\":\"30分钟搞定iOS自定义相机\"},{\"url\":\"https://www.jianshu.com/p/8c7ca1dd7f02\",\"title\":\"iOS-AVFoundation自定义相机详解\"},{\"url\":\"https://stackoverflow.com/questions/588866/whats-the-difference-between-the-atomic-and-nonatomic-attributes\",\"title\":\"ios - What's the difference between the atomic and nonatomic attributes? - Stack Overflow\"},{\"url\":\"https://github.com/skyming/iOS-Performance-Optimization\",\"title\":\"skyming/iOS-Performance-Optimization: 关于iOS 性能优化梳理、内存泄露、卡顿、网络、GPU、电量、 App 包体积瘦身、启动速度优化等、Instruments 高级技巧、常见的优化技能- Get — Edit\"},{\"url\":\"https://github.com/qinjx/30min_guides/blob/master/ios.md\",\"title\":\"30min_guides/ios.md at master · qinjx/30min_guides\"},{\"url\":\"https://stackoverflow.com/questions/5887248/ios-app-maximum-memory-budget\",\"title\":\"iphone - ios app maximum memory budget - Stack Overflow\"},{\"url\":\"https://blog.csdn.net/bxjie/article/details/39581565\",\"title\":\"关于airplay协议实现镜像功能研究\"},{\"url\":\"https://depthlove.github.io/2015/09/16/build-X264-library-for-iOS-platform/\",\"title\":\"编译iOS平台上使用的X264库 | Minmin.Sun Blog\"}]},{\"name\":\"OpenGL ES\",\"sites\":[{\"url\":\"http://toughcoder.net/blog/2018/07/31/introduction-to-opengl-es-2-dot-0/\",\"title\":\"年轻人的第一篇OpenGL ES 2.0教程\"},{\"url\":\"https://blog.csdn.net/kesalin/column/info/opengl-es2-ios\",\"title\":\"OpenGL ES 2.0 iOS教程\"}]},{\"name\":\"Kotlin\",\"sites\":[{\"url\":\"https://www.kotlincn.net/docs/tutorials/android-plugin.html\",\"title\":\"Kotlin Android扩展\"}]},{\"name\":\"多媒体\",\"sites\":[{\"url\":\"http://blog.csdn.net/leixiaohua1020\",\"title\":\"雷神博客\"},{\"url\":\"http://blog.csdn.net/u011270282/article/details/42058617\",\"title\":\"h.264保存flv文件\"},{\"url\":\"http://blog.csdn.net/yeyumin89/article/details/7932368\",\"title\":\"将h.264视频流封装成flv格式文件（一.flv格式）\"},{\"url\":\"http://blog.csdn.net/yeyumin89/article/details/7932431\",\"title\":\"将h.264视频流封装成flv格式文件（二.开始动手）\"},{\"url\":\"https://segmentfault.com/a/1190000007361184\",\"title\":\"FLV文件格式解析\"},{\"url\":\"http://blog.csdn.net/leixiaohua1020/article/details/17934487\",\"title\":\"视音频编解码学习工程：FLV封装格式分析器\"},{\"url\":\"http://www.tuicool.com/articles/vu2Mvy\",\"title\":\"FLV视频封装格式详解\"},{\"url\":\"http://blog.csdn.net/zhuweigangzwg/article/details/25815851\",\"title\":\"音视频同步(播放)原理\"},{\"url\":\"http://developer.t-firefly.com/thread-5731-1-1.html\",\"title\":\"音视频同步和视频帧率控制的原理和实现\"},{\"url\":\"http://blog.csdn.net/Byeweiyang/article/details/78134674\",\"title\":\"实时视频传输的关键技术 H.264 全解析\"},{\"url\":\"http://blog.csdn.net/shangmingyang/article/details/50837852\",\"title\":\"带你吃透RTMP\"},{\"url\":\"http://blog.csdn.net/liuxingen/article/details/45420455\",\"title\":\"字节序(byte order)和位序(bit order)\"},{\"url\":\"https://blog.csdn.net/leixiaohua1020/article/details/11694129\",\"title\":\"RTMP规范简单分析\"},{\"url\":\"https://my.oschina.net/u/213072/blog/52053\",\"title\":\"FLV视频封装格式详解\"},{\"url\":\"https://depthlove.github.io/2015/11/13/flv-analysis-in-rtmp-live-play/\",\"title\":\"rtmp直播推流（一）－－flv格式解析与封装 | Minmin.Sun Blog\"},{\"url\":\"https://blog.csdn.net/sphone89/article/details/17492433\",\"title\":\"h264 profile &amp; level\"},{\"url\":\"http://www.lighterra.com/papers/videoencodingh264/\",\"title\":\"Video Encoding Settings for H.264 Excellence\"}]},{\"name\":\"FFmpeg\",\"sites\":[{\"url\":\"https://blog.csdn.net/leixiaohua1020\",\"title\":\"雷神博客\"},{\"url\":\"https://www.ffmpeg.org/ffmpeg-filters.html#format-1\",\"title\":\"FFmpeg Filters Documentation\"},{\"url\":\"https://blog.csdn.net/fireroll/article/details/8607954\",\"title\":\"为ffmpeg添加自定义滤镜\"},{\"url\":\"https://blog.csdn.net/dancing_night/article/details/46348515\",\"title\":\"把自定义的demuxer加入ffmpeg源码\"},{\"url\":\"https://blog.csdn.net/wstarx/article/details/1572393\",\"title\":\"FFMpeg框架代码阅读\"},{\"url\":\"https://blog.csdn.net/m0_37402140/article/details/77963538\",\"title\":\"ffmpeg八大模块及常用函数介绍\"},{\"url\":\"https://blog.csdn.net/leixiaohua1020/article/details/15811977\",\"title\":\"[总结]FFMPEG视音频编解码零基础学习方法\"},{\"url\":\"https://juejin.im/post/5cad73425188251aee3a5624\",\"title\":\"三、ffplay、ffmpeg日常食用的简单姿势\"},{\"url\":\"https://blog.csdn.net/Contex_A17/article/details/82014826\",\"title\":\"ffmpeg倒放音视频\"},{\"url\":\"https://blog.csdn.net/vbLittleBoy/article/details/8587745\",\"title\":\"关于ffmpeg如何提取视频的关键帧的问题\"},{\"url\":\"https://www.jianshu.com/p/0f2d762c4d1d\",\"title\":\"FFMpeg中seek函数解析\"},{\"url\":\"https://blog.csdn.net/leixiaohua1020/article/details/44220151\",\"title\":\"FFmpeg源代码结构图 - 解码\"},{\"url\":\"https://blog.csdn.net/CHNIM/article/details/80852806\",\"title\":\"ffmpeg实现视频倒播（基于opencv方法）\"},{\"url\":\"https://stackoverflow.com/questions/42257354/concat-a-video-with-itself-but-in-reverse-using-ffmpeg\",\"title\":\"Concat a video with itself, but in reverse, using ffmpeg - Stack Overflow\"},{\"url\":\"https://blog.csdn.net/ternence_hsu/article/details/85865718\",\"title\":\"ffmpeg 视频倍速播放 和 慢速播放\"},{\"url\":\"https://zhuanlan.zhihu.com/p/44615185\",\"title\":\"ffplay音视频同步分析——基础概念\"},{\"url\":\"https://juejin.im/post/5cad790f51882518b87e1404\",\"title\":\"十、详解FFplay音视频同步\"},{\"url\":\"https://blog.csdn.net/lrzkd/article/details/78661841\",\"title\":\"ffplay播放器音视频同步原理\"},{\"url\":\"https://my.oschina.net/u/735973/blog/806117\",\"title\":\"ffmpeg音视频同步---视频同步到音频时钟\"},{\"url\":\"https://superuser.com/questions/1189472/make-a-clip-from-a-video-with-ffmpeg\",\"title\":\"Make a clip from a video with FFMPEG\"}]},{\"name\":\"CMake\",\"sites\":[{\"url\":\"https://www.cnblogs.com/zl1991/p/6526613.html\",\"title\":\"CMakeLists.txt写法\"},{\"url\":\"https://elloop.github.io/tools/2016-04-10/learning-cmake-2-commands\",\"title\":\"CMake 常用命令和变量\"},{\"url\":\"https://blog.csdn.net/a794226986/article/details/18616511\",\"title\":\"cmake处理多源文件目录的方法\"},{\"url\":\"https://stackoverflow.com/questions/52183875/create-cmake-c-c-library-to-android-without-android-studio\",\"title\":\"Create (Cmake) C/C++ Library to Android without Android Studio\"},{\"url\":\"https://owent.net/2017/1405.html\",\"title\":\"用cmake交叉编译到iOS和Android\"},{\"url\":\"https://fucknmb.com/2017/06/27/cmake-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/\",\"title\":\"cmake交叉编译\"},{\"url\":\"https://zhuanlan.zhihu.com/p/24535328\",\"title\":\"更简单的CMake交叉编译\"}]},{\"name\":\"Lua\",\"sites\":[{\"url\":\"https://www.w3cschool.cn/doc_lua_5_3/dict\",\"title\":\"Lua 5.3 词典 速查词典|Lua 5.3 词典 API中文手册|Lua 5.3 词典参考文档_w3cschool\"},{\"url\":\"https://wowwiki.fandom.com/wiki/WoW_AddOn\",\"title\":\"WoW AddOn | WoWWiki | FANDOM powered by Wikia\"},{\"url\":\"https://wowwiki.fandom.com/wiki/UI_FAQ/AddOn_Author_Resources#Resources\",\"title\":\"UI FAQ/AddOn Author Resources | WoWWiki | FANDOM powered by Wikia\"},{\"url\":\"https://bbs.nga.cn/read.php?tid=14839516\",\"title\":\"[狂暴][PVE]8.2争霸艾泽拉斯狂暴专精指南[08/20更新狂暴战8.2版本木桩讲解视频] NGA玩家社区\"},{\"url\":\"http://wowprogramming.com/\",\"title\":\"Home Page - World of Warcraft Programming: A Guide and Reference for Creating WoW Addons\"},{\"url\":\"https://github.com/LewisJEllis/awesome-lua\",\"title\":\"LewisJEllis/awesome-lua: A curated list of quality Lua packages and resources.\"},{\"url\":\"https://luarocks.org/\",\"title\":\"LuaRocks - The Lua package manager\"},{\"url\":\"https://bbs.nga.cn/read.php?tid=17358638\",\"title\":\"[教程] 提取魔兽世界的编码文件和美工文件+替换技能图标+常用工具 NGA玩家社区\"},{\"url\":\"https://warcraft.huijiwiki.com/wiki/%E6%8F%92%E4%BB%B6%E7%BC%96%E5%86%99%E4%BB%8B%E7%BB%8D\",\"title\":\"插件编写介绍 - 魔兽世界中文维基，自由编辑的魔兽资料库 - 灰机wiki\"},{\"url\":\"http://www.waitingfy.com/archives/1047\",\"title\":\"《魔兽世界插件》教程—21点扑克游戏 Blackjack | Waiting For You\"}]},{\"name\":\"其它\",\"sites\":[{\"url\":\"https://deerchao.cn/tutorials/regex/regex.htm\",\"title\":\"正则30分钟教程\"},{\"url\":\"https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html\",\"title\":\"使用hexo+github搭建免费个人博客详细教程\"},{\"url\":\"https://www.zhihu.com/question/28889256\",\"title\":\"哪一句日常用的英语很简单但是格调和品位很高？\"}]}]}","link":"/navi/favorite.json"}],"posts":[{"title":"2020年度总结","text":"2020 年是动荡的一年。不光是宏观的社会因素的层面上，单就我的工作内容来说，也很动荡。 2020 年学习计划大半泡了汤。年初发愿要以最高优先级对待的 StreamingCore 项目也彻底停滞。工作内容跟直播越来越无关，搞得我也没多大心思再去钻研相关技术；还总是犯懒，三天打鱼两天晒网，又喜欢东一锄头西一耙的瞎搞。 上班时…上半年…上半年工作内容集中在 Unity 上，开发 VR 眼镜直播客户端。收获颇丰。 上手了 Unity 开发平台，知道了怎么用 Unity 开发一款 Android APP； 重新捡起 C#。C# 的语法糖一直很香，可惜在 Unity 内应用 C# 还是有很大的局限性，比如 async await 这套还不能用； 对 3D 游戏开发有了初级的实战经验，模型的使用，Draw Call 优化等等。 算是多了条路吧。 但遗憾还是多过收获，下半年这个项目因为没什么用户量被砍掉了，就此也没机会再深入系统地学习 Unity 了。 下半年…下一个项目是我们组自研引擎的可视化编辑器，等于从使用 Unity “提升”到了开发 Unity。我个人并不看好这个立意宏伟的大饼，因为从一开始其投入的人力就不足，一直持续到现在。 好歹工作内容还是我喜欢的，又捡起了 Qt。 架构设计、模块划分是我独立完成的，支撑能力尚可，至今还没遭遇需要大修改的需求。 一些基础组件的技术选型和开发落地也是我完成的，比如打包器、资源管理器。一个很严重的失误就是直接套用了 Qt Property Browser 实现的属性面板。UI 风格定制化能力极差，源码改动很多；自定义属性交互也极其麻烦，很典型的过度设计。 下班时…爬虫搞 Unity 搞的心痒痒，想自己搞个小游戏。什么样的游戏呢？福利游戏吧。就开始了找资源之路。手上一把的 Jav 网站，挑了一个简单、顺眼的开始学写爬虫。写到后来，已经完全忘记了初衷，全心全意地投入到爬虫事业中了。 现在爬虫部署到了我的服务器上，可以多线程操作，可以根据关键字过滤爬取，还可以分门别类爬取。技术学到了，身体也跨了。 视频剪辑器爬下来的 Jav，太大太长，不利于反复温习其中精彩部分，就又开始搞剪辑。 把 FFmpeg 里 format、codec、filter 几个模块 api 的基础用法温习、学习后，封装串联，用 Qt 实现了 UI 交互。 现在已经基本满足了需求，能实现精确到帧到帧的视频截取。 总结的总结…人总说“试试那人的深浅”，没人说“试试那人的宽窄”，可见精深某一领域是更难得的能力。我好像怎么也成不了这种人，且就随遇而安，这么动荡着吧。","link":"/2020%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93.html"},{"title":"2020 年学习规划","text":"2020年会整体围绕StreamingCore项目运作。先期会先做一部分复习工作，比如C++相关、工程相关；其它所列知识点会边学边应用。 尽量详细地列出各项，不断回顾并更新进度。 理论知识复习C++ 《C++ Primer》 ✅ 《Effective Modern C++》 ❌ 《C++标准库》 ✅ 《Effective STL》 ❌ 复习《设计模式》 ✅复习《Kotlin语言官方参考文档》 ✅学习《Swift编程权威指南》 ❌多媒体技术FFmpeg应用能力 ❌ 这里简述成果 OpenGL ES应用能力 ❌ 这里简述成果 Android平台多媒体接口应用 MediaCodec ❌ Camera ❌ Camera2 ❌ iOS平台多媒体接口应用 Audio Unit ❌ AVCapture ❌ ReplayKit ❌ AudioToolBox ❌ VideoToolBox ❌ libx264应用能力 ❌","link":"/2020%E5%B9%B4%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%92.html"},{"title":"AVX2 优化 C++ 混响算法","text":"这里 是一份 C++ 混响算法的实现，修正掉索引下溢 BUG 后大概是这样： 123456789size_t samples = file.file_size / 4;size_t ir_samples = sizeof(ir) / 4;for (size_t i = 0; i &lt; samples; ++i){ for (size_t j = 0; j &lt; ir_samples &amp;&amp; i &gt;= j; ++j) { file.out_file_data[i] += file.in_file_data[i - j] * ir[j]; }} 在我的 Intel i7-7700 4 核机器上，混响一段 60s、单声道、float 采样的 pcm 音频需要 12452 ms 的时间。 内层循环是一个一维卷积，累加当前采样点之前所有采样点的混响（乘法）效果，展开后寻找规律： 1234567891011121314151617181920212223242526out[0] = in[0] * ir[0]out[1] = in[1] * ir[0] + in[0] * ir[1] out[2] = in[2] * ir[0] + in[1] * ir[1] + in[0] * ir[2] ......out[7] = in[7] * ir[0] + in[6] * ir[1] + in[5] * ir[2] + in[4] * ir[3] + in[3] * ir[4] + in[2] * ir[5] + in[1] * ir[6] + in[0] * ir[7]......out[n] = in[n] * ir[0] + in[n-1] * ir[1] + ... + in[0] * ir[n] AVX2 一次处理 8 个 float 数据，所以前 7 个采样的混响需要用 0 补齐，7 之后的采样就可以自后往前每次处理 8 个采样的混响和累加了；j 的步进也应该从 1 变成 8。 优化后，代码变成这个样子： 1234567891011121314151617181920212223242526272829303132333435363738394041size_t samples = file.file_size / 4;size_t ir_samples = sizeof(ir) / 4;for (size_t i = 0; i &lt; samples; ++i){ __m256 out = { 0 }; for (size_t j = 0; j &lt; ir_samples &amp;&amp; i &gt;= j; j += 8) { __m256 mm256_ir = _mm256_load_ps(ir + j); __m256 in; switch (i - j) { // _mm256_set_ps 以逆序加载数据，即第一个参数对应 out.m256_f32[7]，最后一个参数对应 out.m256_f32[0] case 0: in = _mm256_set_ps(0, 0, 0, 0, 0, 0, 0, file.in_file_data[0]); break; case 1: in = _mm256_set_ps(0, 0, 0, 0, 0, 0, file.in_file_data[0], file.in_file_data[1]); break; case 2: in = _mm256_set_ps(0, 0, 0, 0, 0, file.in_file_data[0], file.in_file_data[1], file.in_file_data[2]); break; case 3: in = _mm256_set_ps(0, 0, 0, 0, file.in_file_data[0], file.in_file_data[1], file.in_file_data[2], file.in_file_data[3]); break; case 4: in = _mm256_set_ps(0, 0, 0, file.in_file_data[0], file.in_file_data[1], file.in_file_data[2], file.in_file_data[3], file.in_file_data[4]); break; case 5: in = _mm256_set_ps(0, 0, file.in_file_data[0], file.in_file_data[1], file.in_file_data[2], file.in_file_data[3], file.in_file_data[4], file.in_file_data[5]); break; case 6: in = _mm256_set_ps(0, file.in_file_data[0], file.in_file_data[1], file.in_file_data[2], file.in_file_data[3], file.in_file_data[4], file.in_file_data[5], file.in_file_data[6]); break; default: in = _mm256_set_ps(file.in_file_data[i - j - 7], file.in_file_data[i - j - 6], file.in_file_data[i - j - 5], file.in_file_data[i - j - 4], file.in_file_data[i - j - 3], file.in_file_data[i - j - 2], file.in_file_data[i - j - 1], file.in_file_data[i - j]); break; } out = _mm256_fmadd_ps(in, mm256_ir, out); } file.out_file_data[i] = out.m256_f32[0] + out.m256_f32[1] + out.m256_f32[2] + out.m256_f32[3] + out.m256_f32[4] + out.m256_f32[5] + out.m256_f32[6] + out.m256_f32[7];} 耗时优化到了 1285 ms，性能提升了约 9.7 倍，超过了 AVX2 8 倍的理论时间上限，也可见编译器对 switch 分支优化的厉害程度。 完整测试项目在这里。","link":"/AVX2%E4%BC%98%E5%8C%96C++%E6%B7%B7%E5%93%8D%E7%AE%97%E6%B3%95.html"},{"title":"C++17 并行算法探究","text":"C++17 终于把并行计算引入到了 STL，且更新成本极低，只是把 大多数迭代算法函数 多加了一个并行版本的重载，实现了并行、并发两个维度上的性能优化。 比如，对一组数据做一个加倍的 transform： 串行版本，同以前：1std::transform(std::execution::seq, iter_begin, iter_end, output_iter, [](int val) { return val * 2; }); 并行版本，多以线程池方式实现： 1std::transform(std::execution::par, iter_begin, iter_end, output_iter, [](int val) { return val * 2; }); 并行 + 并发版本，同时以线程池和 SIMD 相关方法实现： 1std::transform(std::execution::par_unseq, iter_begin, iter_end, output_iter, [](int val) { return val * 2; }); 单独的并发版本 std::execution::unseq 已经加入到 C++20 标准。 但是，C++17 并行算法离可实际应用还差了十万八千里。 首先，编译器实现进度不理想。LLVM 彻底没展开此项工作 ；MSVC 仅完成部分算法的并行实现，即 std::execution::par_unseq 和 std::execution::par 完全等效，还是个半成品，常用的如 replace_copy_if 甚至并行版本都没实现；三去其二，GCC 即便完全支持，对跨平台开发的意义也不是很大了。 其次，如前所述，单独的并发版本是在 C++20 标准化的。个人认为这个才是杀器，应用面最广。并行要考虑线程切换、共享数据加锁等等负面因素，对一些小而频繁的数据加工，甚至有副作用。比如，直播音效的实现，为了保证低延时根本不允许缓存大块数据后再处理。并发的应用粒度小得多，对直播等场景不会造成负面影响。这大概也是编译器厂商不是那么热心的缘故之一吧…… 一句话总结，C++17 并行算法暂时没必要关注。","link":"/C++17%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95%E6%8E%A2%E7%A9%B6.html"},{"title":"FFmpeg 编码 H264 视频码率过高问题解决方法","text":"转码 mp4 的时候发现，输出的 H264 视频流码率高达 10M kbps+，而源码率只有约 2M kbps，这明显是不对的。明明已经通过 AVCodecContext::bit_rate 字段设定了码率却没有生效。 通过这篇 wiki，找到了答案。H264 有一套很复杂的码控规范，不能简单通过一个固定的码率数值指定输出码率——H264 基本上做不到这么精确的码控。推荐做法是启用 CRF 码控，设置合适的压缩率： 123456// 启用 CRF 码控// CRF 参数值取值范围为 [0,51]。0 为无损，51 最差；23 是 libx264 默认值。if (codec-&gt;id == AV_CODEC_ID_H264){ av_opt_set_int(_context, &quot;crf&quot;, 23, AV_OPT_SEARCH_CHILDREN);} 在 avcodec_open2() 之前加入上边代码块，输出视频流码率下降明显。","link":"/FFmpeg%E7%BC%96%E7%A0%81H264%E8%A7%86%E9%A2%91%E7%A0%81%E7%8E%87%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.html"},{"title":"C++17 几个新的语言特性","text":"1. 结构化绑定声明绑定指定名称到初始化器的子对象或元素。 通俗地讲，对形如 std::pair、std::tuple 或自定义聚合类型的实例，可以声明一组变量直接指向其成员，无需创建一个对应聚合类型的临时变量： 1234std::pair&lt;int, int&gt; point{ 0, 1 };// some operations around point// ...auto [x, y] = point; // x equals to point.first; y equals to point.second 引用语法同样适用： 1234// keys to valuesstd::pair&lt;std::vector&lt;int&gt;, std::vector&lt;int&gt;&gt; kv;// ...auto&amp;&amp; [keys, values] = kv; 不过不能绑定到已声明变量上，还是要通过 std::tie() 绑定： 1234int x = 0, y = 0;std::pair&lt;int, int&gt; point{ 0, 1 };[x, y] = point; // 不支持这种语法std::tie(x, y) = point; // Ok 2. if &amp; switch 内初始化语句类似 for 循环的语法：if (init; condition)、switch (init; condition) 可以将 init 语句内声明的变量的生命周期限制在 if else、switch case 块内。优点显而易见，再也不用在外层作用域预先声明变量使其在 if else 内都可见了。极大程度上避免了名字污染问题。 1234if (const auto pos = myString.find(&quot;World&quot;); pos != std::string::npos) std::cout &lt;&lt; pos &lt;&lt; &quot; World\\n&quot;;else std::cout &lt;&lt; pos &lt;&lt; &quot; not found!!\\n&quot;; 3. 类模版的模版参数推断模版参数类型推断终于普及到类模板了。在这之前，创建一个 std::pair 对象最常用的方法不是直接构造，而是通过 std::make_pair() 函数： 1auto myPair = std::make_pair(42, &quot;hello world&quot;); 为的就是利用函数模板的参数类型推断能力，可以少敲几下键盘。现在，类模版同样可以推断参数类型了： 12345// before: // std::pair&lt;int, const char*&gt; myPair{ 42, &quot;hello world&quot; }// now:std::pair myPair{ 42, &quot;hello world&quot; }; 对其它所有类模版同样适用： 12345// before:// std::array&lt;int, 3&gt; arr{ 1, 2, 3 };// now:std::array arr{ 1, 2, 3 }; 12345// before:// std::lock_guard&lt;std::mutex&gt; _(mtx);// now:// std::lock_guard _(mtx); 4. 折叠表达式展开一个参数包并对其中参数依次施以动作。算是对 C++11 里引入的变参模版的应用规则的一种优化吧。 折叠表达式支持 4 种形式的语法表达： ( pack op ... ) ( ... op pack ) ( pack op ... op init ) ( init op ... op pack ) 这里有详细的解释，比我在这胡说八道强多了。 一个经典的应用场景是日志打印： 12345678// 第四种语法的表现template &lt;class... Args&gt;void print(Args&amp;&amp;... args){ (cout &lt;&lt; ... &lt;&lt; std::forward&lt;Args&gt;(args)); // 注意：整个表达式以 () 包含。}print(1, 3, &quot;hello&quot;); // 打印出：13hello 输出的参数全挤在一起，这肯定不是想要的结果。可以改造下： 12345678// 第一种语法的表现template &lt;class... Args&gt;void print(Args&amp;&amp;... args){ ((cout &lt;&lt; std::forward&lt;Args&gt;(args) &lt;&lt; &quot; &quot;), ...);}print(1, 3, &quot;hello&quot;); // 打印出：1 3 hello 逗号左边两次输出以 () 包含，变成了一个表达式，等同于第一种语法的 pack 部分；‘,’ 即 op。整个表达式的意思就是对参数包内所有参数，先打印参数本身再打印一个空格符，依次操作。 参考资料：《C++17 in detail》 structured binding declaration fold expression","link":"/C++17%E5%87%A0%E4%B8%AA%E6%96%B0%E7%9A%84%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7.html"},{"title":"NEON 优化 C++ 混响算法","text":"普通的串行版本 和优化思路在这里。 跟 AVX2 不一样的大概是两点： NEON 指令只可以同时处理 4 个 float 数据 NEON 有自己的函数集 优化后的 NEON 版本： 12345678910111213141516171819202122232425262728293031323334353637383940414243size_t samples = file.file_size / 4;size_t ir_samples = sizeof(ir) / 4;for (size_t i = 0; i &lt; samples; ++i){ float32x4_t out = { 0 }; for (size_t j = 0; j &lt; ir_samples &amp;&amp; i &gt;= j; j += 4) { float32x4_t ir_x4 = vld1q_f32(ir + j); float32x4_t in; switch (i - j) { case 0: { float temp[4] = {file.in_file_data[0], 0, 0, 0}; in = vld1q_f32(temp); break; } case 1: { float temp[4] = {file.in_file_data[1], file.in_file_data[0], 0, 0}; in = vld1q_f32(temp); break; } case 2: { float temp[4] = {file.in_file_data[2], file.in_file_data[1], file.in_file_data[0], 0}; in = vld1q_f32(temp); break; } default: // float temp[4] = {file.in_file_data[i-j], file.in_file_data[i-j-1], file.in_file_data[i-j-2], file.in_file_data[i-j-3]}; // in = vld1q_f32(temp); in = vld1q_f32(file.in_file_data + i - j - 3); in = vrev64q_f32(in); break; } out = vmlaq_f32(out, in, ir_x4); } file.out_file_data[i] += vgetq_lane_f32(out, 0); file.out_file_data[i] += vgetq_lane_f32(out, 1); file.out_file_data[i] += vgetq_lane_f32(out, 2); file.out_file_data[i] += vgetq_lane_f32(out, 3);} NEON 函数集跟 AVX2 略有不同，只有顺序加载的 load ( vld1q_f32() )，没有逆序、离散加载的 set，所以需要构造一个临时的 float 数组，逆序赋值，再传入 load 函数。 不需要补零时，构造临时 float 数组再 load，效率明显低于上边代码示例里 default 块的实现方式，即先一次性 load 4个数据后再反转。在我的测试用例里，单这一个改动就提升了约 10% 的性能。 完整的 Android 测试工程在这里","link":"/NEON%E4%BC%98%E5%8C%96C++%E6%B7%B7%E5%93%8D%E7%AE%97%E6%B3%95.html"},{"title":"Python3 爬虫初体验","text":"数据提取通过 PyCharm 的包管理工具分别下载安装 requests 、bs4 、lxml 库。 requests 库requests 是一个简洁优雅的 HTTP 库，基于 urllib3 再封装。一个 get 请求用一行代码即可实现： 1res = requests.get('https://github.com/') 许多网站会根据 Request Header 识别请求是否来自非爬虫应用，可以通过伪造 Header 骗过，比如，把自己伪装成一个 Chrome 浏览器： 1234headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'}res = requests.get('https://github.com/', headers=headers) 可以通过浏览器的开发者工具获取到完整的头信息列表，做到完美伪装。 BeautifulSoup 和 lxml 库bs4 即 BeautifulSoup ，是一个可以从 HTML 或 XML 文件中提取数据的库。它支持以多种形式遍历、查找 HTML 内容，比如 CSS 的选择器、标签类型甚至正则表达式。BeautifulSoup 可以配合第三方 HTML 解析器使用，比如 lxml 。lxml 运行效率比标准库高，大概算是事实标准库了吧。 基于上述准备工作，即可完成大部分的静态页面内容抓取，比如，获取 GitHub 首页的 body 内容： 123456import requestsfrom bs4 import BeautifulSouprsp = requests.get('https://github.com/')soup = BeautifulSoup(rsp.text, 'lxml')print(soup.find('body')) # 打印出网页 body 内容 配合浏览器的开发者工具，可以拿到网页所有标签的 css selector，据此可以提取出更精细的数据，比如，拿到 GitHub 用户的头像： 12345rsp = requests.get('https://github.com/liangzuoting')soup = BeautifulSoup(rsp.text, 'lxml')img = soup.select('#js-pjax-container &gt; div.container-xl.px-3.px-md-4.px-lg-5 &gt; div &gt; div.flex-shrink-0.col-12.col-md-3.mb-4.mb-md-0 &gt; div &gt; div.clearfix.d-flex.d-md-block.flex-items-center.mb-4.mb-md-0 &gt; div.position-relative.d-inline-block.col-2.col-md-12.mr-3.mr-md-0.flex-shrink-0 &gt; a &gt; img')if len(img) &gt; 0: print(img[0].get('src')) # 打印出头像图片 url BeautifulSoup.select 函数接受一个 css selector 字符串，并返回匹配列表。 数据存储通过 Python 内置的 csv 库，可以很方便的将提取的数据写入到 csv 文件。 1234567import csvf = open('d:\\dd.csv', mode='w', newline='\\n', encoding='utf-8-sig')w = csv.writer(f)w.writerow(['1', '二,三,四', '5\\n6\\n7'])w.writerow(['11', '22,33,44', '55\\n66\\n77'])f.close() 务必显式指定 newline 参数为 \\n，Windows 平台下默认用 \\r\\n 做换行符，在 excel 里打开 csv 文件后出现空白行。 以 utf-8-sig 编码创建文件，能解决非 ascii 码字符乱码问题，sig 即 BOM，显式地把字节序写入到了文件里。 以换行符插入项字符串内，可实现单元格内换行。对以列表存储的动态数据，需要定义自己的字符串格式化函数： 12def ls_to_str(ls): '\\n'.join(map(str, ls)) map 函数把 ls 列表项通过 str 函数依次映射成字符串，这个在列表项是自定义类型时是必须的，否则列表项只记录其地址值。 小结爬虫入门还是很简单的，Python 完整的生态链功不可没。问题总是出在意想不到的地方，比如 Chrome 的弹窗自动拦截。通过开发者工具获取到的 selector 是拦截后页面的层级关系，导致 BeautifulSoup.select 获取的数据总是对不上。可以关闭浏览器的相关功能；或从 requests 导出完整的页面内容，保存成 HTML 文件后再解析。","link":"/Python%E7%88%AC%E8%99%AB%E5%88%9D%E4%BD%93%E9%AA%8C.html"},{"title":"StreamingCore 项目介绍","text":"是时候驻足整装了明年的业余时间会集中在这个StreamingCore项目上。进入直播行业也有六七年时间了，这将是我迄今的职业生涯中最重要的一份总结。 如同其名，StreamingCore将是一套直播核心流程的解决方案，集成采集、编码、推流功能，同时支持Android和iOS两端。借鉴FFmpeg的设计思想，StreamingCore也会以模块化架构实现，允许独立使用某一功能模块。同一功能模块，或许会提供多套实现方案，以供比较选择。 暂拟定的功能模块音频采集模块 OpenSL ES implementation for Android AudioRecord implementation for Android Audio Unit implementation for iOS 图像采集模块 Implementation based on Camera interface for Android Implementation based on Camera2 interface for Android Implementation based on Virtual Display for Android Implementation based on AVFoundation framework for iOS Implementation based on ReplayKit for iOS 音频编码模块 Audio Encoder implementation based on MediaCodec for Android Audio Encoder implementation based on AudioToolBox for iOS 视频编码模块 Video Encoder implementation based on MediaCodec for Android Video Encoder implementation based on VideoToolBox for iOS 推流模块 Streaming by Rtmp Protocol Streaming by custom Protocol based on UDP 各个模块将暴露标准化接口，供扩展实现。 我不喜欢Java和Objective-C或者反过来说，我喜欢Kotlin和Swift。够简洁，够现代化，这就足够了。公司业务已呈现尾大不掉、积重难返之态，更加之KPI当头，相关负责人没有勇气切换到新的开发环境。我已学的Kotlin，我想学的Swift，几无用武之地，这也是我萌生实施此项目的一个重要原因。 同样基于这个技术决定，项目质量无从保证。成品后能否被接入方接受，就更难说了…… 大概的开发计划 时间周期为一年（2020.1.1——2020.12.31） 先Android端全功能，再iOS端","link":"/StreamingCore%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D.html"},{"title":"使用 Unity 开发 Android VR 应用系列之一——开发环境设置","text":"0. 读者定位 熟悉 Android 开发 对 C#/.NET 有一定了解 想学习如何通过 Unity 开发 Android 应用 Unity 是微软旗下的跨平台游戏/应用开发工具，C# 自然成为其首推支持的前端开发语言。C# 虽是从 Java 脱胎而出，经过近二十年的独立发展，已经成长为一门明显优于 Java 的现代化语言（这当然是我个人的主观见解）。我也发愿以此项目为起点，重新掌握 C#/.NET 的技术体系。 1. 开发环境一览 Windwos 10, 16 GB RAM, 1060 3GB+ 独显 Unity 对硬件具有一定要求，不仅指构建版本，开发过程中还需要频繁使用开发机运行调试项目，硬件配置对开发效率的影响还是比较大的。 Android Studio 环境（包括 SDK 和 NDK） 不确定 Unity 兼容所有 Android Studio 版本环境——毕竟 Android Studio 本身就存在比较显著的版本兼容性问题。所以如果你碰到项目配置相关的错误，建议尝试提高或降低 Android Studio/SDK/NDK 版本来解决。我个人的 Android Studio 环境是 Android Studio 3.5.3/SDK Platform 26/NDK r16b。 Unity 2018.4.14f1 Unity 存在多个并行版本，包括 2017/2018/2019 等系列。大版本间互不兼容，所以立项之初 Unity 版本选择是一个重要任务。版本选择大概有几个原则： 依赖方的要求，很多第三方 SDK 不保证兼容所有 Unity 版本 优先使用 LTS (Long Term Support) 版本，LTS 意味着官方的持续维护，版本质量更高 如果想体验所有新功能，优选 2019 版本；如果更注重产品的稳定性，优选 2018 版本。LTS 并不意味着会同步所有最新版本的大功能，所以 2018 LTS 的功能是弱于 2019 系列的 Visual Studio Community 2019 此项可选。通过一些渠道安装 Unity，会同时包含 Visual Studio Community 2017 的安装选项，这已经足够了。 Pico G2 4K 眼镜 我们应用的适配机型之一，我个人的最爱 2. 首次运行设置1) Unity 关联 Android 环境 启动 Unity，依次选中菜单项 Edit - Preferences - External Tools，显示如下: 按实际安装路径分别填充标注的两个选项。没有用到 NDK 时，此项可空。 2) Android App 构建配置 在 Unity 中依次选中菜单项 Edit - Project Settings，在弹出窗口中的左栏选中 Player 选项后，在右栏切换到 Android settings 标签页。 选项繁多，不一一介绍。强烈建议对照文档熟练掌握所有选项含义，这是十分有意义的。这里仅介绍几个”高级”选项。 2.1) 自定义 Gradle 脚本 选中 Publishing Settings 下拉菜单，勾选如上图标识选项。Unity 默认使用对用户隐藏的 gradle 脚本构建 App，使用自定义 gradle 脚本，给开发者提供了从更细粒度上干预构建过程的能力。 2.2) C# 脚本配置 在 Other Settings 下拉菜单中，按上图所示设置各选项。将 .NET 运行时版本提升到 4.x 的原因是可以使用 C# 6.0 特性。 3) Unity 关联 Visual Studio Preferences 窗口中的 External Tools 标签页中，有如下选项 如果你的 Visual Studio 是通过 Unity 安装器打包安装，这里已经自动关联；如果想手动切换到其它 Visual Studio 实例，可以通过点击这里，浏览本地安装目录，找到并选中 devenv.exe 即可。如果这里没有配置正确，在 Visual Studio 中打开脚本文件后将不能成功自动感知，且不能通过 “附加到 Unity” 功能实时调试脚本。虽然不影响脚本的正常编译，对开发效率无疑是巨大打击。 3. 后记文中所记并非一个完整的 “Unity 下开发 Android App 配置流程”，此种教程文档网络上已数不胜数，我也是参照此类文章迈出的第一步。此文的价值，恰恰在于它是对此类千篇一律的入门流水账的一个重要补充，是我开发过程中一些痛苦教训的精炼提取，可以让你的第一步迈得更扎实。叙事的同时，也掺杂了很多我个人的理解。限于认知，如有冒犯或错误或词不达意等负面情绪传递，凡此种种，请留言指正，谢谢。","link":"/Unity%E5%BC%80%E5%8F%91Android-VR%E5%BA%94%E7%94%A8%E4%B8%80%E2%80%94%E2%80%94%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE.html"},{"title":"在 Unity 项目中使用 OpenFileDialog 实现选择文件功能","text":"开发阶段用的是 UnityEditor.EditorUtility.OpenFilePanel() 函数，简单易用。但是如其名所示，UnityEditor 只在 Unity 编辑器内可见，尝试 build 出 Windows 下 standalone 包时会报编译错误。 解决办法是用 mono 库下的相应功能实现打开文件。 在 Unity 安装根目录的 \\Editor\\Data\\Mono\\lib\\mono\\2.0\\ 路径下找到并复制 System.Windows.Forms.dll 粘贴 dll 文件到 Unity 项目的 Assets\\Plugins\\ 目录 PlayerSettings 中 Api Compatibility Level 选项值从默认的 .NET Standard 2.0 调整为 .NET 4.x 此时，dll 在 Unity 项目中变为可见，可以在脚本文件中使用 dll 中任意功能了。 选择文件对话框通过 System.Windows.Forms.OpenFileDialog 类实现，一个典型用法如下： 12345678System.Windows.Forms.OpenFileDialog ofd = new System.Windows.Forms.OpenFileDialog();ofd.Filter = &quot;Video Files(*.mp4;*.flv)|*.mp4;*.flv&quot;;ofd.RestoreDirectory = true;if (ofd.ShowDialog() != System.Windows.Forms.DialogResult.OK){ return; // user clicked cancel.}var filePath = ofd.FileName; //filePath holds the file's full path. 具体用例参考我的 XOPlayer项目 issues: mono 实现的 OpenFileDialog 太丑了……是非基于宿主系统原生实现的。我用的 Unity 2018.4.14f1 提供的是 mono 2.0 版本，不知是否跟 mono 版本过旧有关。","link":"/Unity%E4%B8%AD%E7%94%A8OpenFileDialog%E5%AE%9E%E7%8E%B0%E9%80%89%E6%8B%A9%E6%96%87%E4%BB%B6%E5%8A%9F%E8%83%BD.html"},{"title":"使用 Unity 开发 Android VR 应用系列之三——实现场景管理器","text":"0. 需求分析绝大多数应用里，都会有类似的业务逻辑：从 A 页面跳转到 B 页面，执行一系列操作后返回 A 页面。 这看似寻常的操作流程，Unity 是不能天然支持的。Unity 的 SceneManager 和 Android 的 activity stacks 在”页面”管理策略上大概有以下不同： 加载 B 场景，默认会销毁 A 场景 这种情况下想实现返回功能，必须重新加载 A，并且还原 A 销毁前的所有状态（滚动条位置等等）——业务逻辑越复杂，要记住的状态越多，这个方案不具有普适性。可显式指定以 Additive 模式加载新场景，加载 B 后仍将 A 保留。但是： Additive 模式加载 B 后，A 不会被隐藏 绝大多数情况下，这也不是我们期望的行为。所以需要基于 Additive 逻辑更进一步，使 B 加载后 A 自动隐藏。 综上，SceneManager 并不满足常见需求，需要自己实现一个 “scene stack”，使多个场景形成 LIFO 的栈式结构。 1. 系统设计以一组类图表示各参与者及其关系： SceneStack 以链表保存场景上下文信息，负责场景的加载与销毁。 注意 SetFirstScene(string sceneName, ControllerBase controller) 函数。应用中第一个场景是由系统负责加载的，用户没有干预的余地，所以只能在其加载后通过此函数手工添加到栈里。 ControllerBase Controller 即是最上层 UI 元素（通常是一个 Canvas）关联的脚本（MonoBehaviour）组件。将 controller 同对应的场景同时保存到 SceneStack 中，即可通过 controller 函数控制场景的显示与隐藏。 如此，基本实现了”新场景加载时使旧场景隐藏而不销毁”的流程化、自动化。 2. 代码实现完整的代码实现在这里，同时提供一个两个场景的简单测试用例。 3. 总结其实，我一直在混用”场景”与 “UI” 的概念。以 MVC 模型解构 Unity 的 UI 系统，场景是 Controller，UI 是 View。所以准确地说，我上述所提到的显示/隐藏，目标不是场景，而是场景内的 UI 元素。 实际也确实如此，Scene 类是没有类似显示/隐藏的接口可用的，代码里操作的是场景内 UI 根元素。我之所以如此表述，只是单纯地想简化描述罢了——毕竟隐藏了所有 UI，就好像是场景隐藏了一样。","link":"/Unity%E5%BC%80%E5%8F%91Android-VR%E5%BA%94%E7%94%A8%E4%B8%89%E2%80%94%E2%80%94%E5%AE%9E%E7%8E%B0%E5%9C%BA%E6%99%AF%E7%AE%A1%E7%90%86%E5%99%A8.html"},{"title":"使用 Unity 开发 Android VR 应用系列之二——使用 Animator 实现鼠标悬浮 UI 缩放动画效果","text":"0. 概览展示效果如下 完整的代码和示例在这里 实现上图所示的动画效果，涉及两个 Unity 知识点： Animator 组件 动态调整 UI 渲染顺序 1. 使用 Animator 实现缩放动画Animator 是一套灵活度跟复杂度同样高的动画系统。简单来讲，其运作原理是：定义若干 Animation Clip，将其在 Animator Controller 中通过 Transition 拼装成一个状态机；创建若干 Parameters，用户通过为其赋不同值控制状态转移。 套用到本例来说，就是： 创建两个 Animation Clip，一个用于放大，一个用于缩小 创建一个 Animator Controller，在其中创建两个状态，分别对应上边两个 Animation Clip 创建一个 bool 参数 hovering，射线进入时为 true，射线移开时为 false 创建两个 Transition，分别使 hovering = true 时执行放大动画，hovering = false 时执行缩小动画 最终，Animator Controller 中呈现如下的状态图： 2. 使用 Canvas 组件实现动态调整渲染顺序默认的 UI 渲染顺序是按其在场景中出现的先后顺序定义的。带来的一个问题是：当对靠前的 UI 做放大动画时，有可能和后面的 UI 重叠，导致当前选中 UI 被遮挡。 可以通过为 UI 添加 Canvas 组件解决此问题。 注意图中的 Graphic Raycaster 组件，需要同 Canvas 成对出现，否则 UI 不能正常处理射线事件。 内嵌的 Canvas 具有上图所示的属性。勾选 Override Sorting 后，通过修改 Order in Layer 项值即可自定义当前 UI 的渲染顺序，值越大，渲染顺序越靠后。 脚本代码如下： 123456789101112// 添加 Canvas 和 GraphicRaycaster 组件if (topWhenHover) // 仅在需要时动态添加相应组件{ if (GetComponent&lt;Canvas&gt;() == null) { gameObject.AddComponent&lt;Canvas&gt;(); } if (GetComponent&lt;GraphicRaycaster&gt;() == null) { gameObject.AddComponent&lt;GraphicRaycaster&gt;(); }} 1234567891011// hover 状态变化时private void OnHoverChanged(bool hover){ if (topWhenHover) { var canvas = GetComponent&lt;Canvas&gt;(); canvas.overrideSorting = hover; canvas.sortingOrder = sortingOrder; } ...} 3. 把所有功能封装到一个自定义组件中可以实现一个高度封装的组件：用户只需要添加此组件，即可自动监听射线进出事件并执行动画。 1. 使组件能接收射线进出事件12345678910111213// 实现对应接口public class HoverScaler : MonoBehaviour, IPointerEnterHandler, IPointerExitHandler{ public void OnPointerEnter(PointerEventData eventData) { OnHoverChanged(true); } public void OnPointerExit(PointerEventData eventData) { OnHoverChanged(false); }} 2. 添加 Animator 组件到 UI123456void Start() { animator = gameObject.AddComponent&lt;Animator&gt;(); var controller = Resources.Load&lt;RuntimeAnimatorController&gt;(&quot;Animations/ScaleWhenHover&quot;); animator.runtimeAnimatorController = Instantiate(controller); } 完整的组件实现在这里 4. Issues 修改 Canvas 的 sortingOrder 属性，会使其父 UI 的 mask 功能失效（demo 有演示）","link":"/Unity%E5%BC%80%E5%8F%91Android-VR%E5%BA%94%E7%94%A8%E4%BA%8C%E2%80%94%E2%80%94Animator%E5%AE%9E%E7%8E%B0%E9%BC%A0%E6%A0%87%E6%82%AC%E6%B5%AEUI%E7%BC%A9%E6%94%BE%E5%8A%A8%E7%94%BB%E6%95%88%E6%9E%9C.html"},{"title":"使用 Unity 开发 Android VR 应用系列之四——用 ScriptableObject 实现预加载","text":"ScriptableObject 是一个数据容器类，可以用它来存储与类实例无关的数据。ScriptableObject 一个主要用途是利用它存储共享资源（比如 Prefab），减少无谓的复制，降低内存使用。 比如说，场景 A 和场景 B 都需要一个登录对话框。常规做法是创建一个 Prefab，分别在 A 和 B 中各自实例化；为了节省内存，可以在 ScriptableObject 中实例此 Prefab，在运行时将此 Prefab 实例添加到 A/B 场景中。 ScriptableObject 最典型的一个应用场景，便是用它来实现预加载了。 如上图，在 Player Settings 的 Optimization 分类下有一个 Preloaded Assets 数组选项。数组列出的 .asset 类型的资源会在程序启动时加载，并一直存活到程序退出。 .asset 资源对应的就是 ScriptableObject。实现 ScriptableObject 类时，需使用 CreateAssetMenu 属性为此 ScriptableObject 在 Assets 菜单中创建一个使用入口： 123456789101112131415// 模仿 lua，创建一个名字为 G 的类，意指此类为 Global used.[CreateAssetMenu(fileName = &quot;GlobalAsset&quot;, menuName = &quot;GlobalAsset&quot;)]public class G : ScriptableObject{// 实例被加载后会触发此回调private void OnEnable(){// 缓存材质资源...// 预先创建对象池...}} 上边的示例代码在 Assets-Create 菜单栏下创建了一个 GlobalAsset 菜单项，通过此菜单项便可方便地创建一个 GlobalAsset.asset 的资源文件。将此资源文件添加到预加载数组中后，程序启动时就会自动实例化一个 G 类实例，实例在被加载后会触发 OnEnable 消息，可以在此回调中实现具体的预加载逻辑。 简言之，ScriptableObject 是 Unity 提供的可完美取代 Singleton 的内建数据共享机制。","link":"/Unity%E5%BC%80%E5%8F%91Android-VR%E5%BA%94%E7%94%A8%E5%9B%9B%E2%80%94%E2%80%94ScriptableObject%E5%AE%9E%E7%8E%B0%E9%A2%84%E5%8A%A0%E8%BD%BD.html"},{"title":"VS2019 编译 QtPropertyBrowser 源码","text":"尝试在 VS 2019 中导入 QtPropertyBrowser 源码到 VS 工程（vcxproj）进行编译时，遇到如下编译错误： 12345671&gt;moc_qtpropertybrowser.cpp1&gt;E:\\QtPropertyBrowserDemo\\QtPropertyBrowserDemo\\x64\\Debug\\moc\\moc_qtbuttonpropertybrowser.cpp(94,29): error C2027: 使用了未定义类型“QtButtonPropertyBrowserPrivate”1&gt;E:\\QtPropertyBrowserDemo\\QtPropertyBrowserDemo\\qtpropertybrowser\\qtbuttonpropertybrowser.h(47): message : 参见“QtButtonPropertyBrowserPrivate”的声明1&gt;E:\\QtPropertyBrowserDemo\\QtPropertyBrowserDemo\\x64\\Debug\\moc\\moc_qtbuttonpropertybrowser.cpp(95,29): error C2027: 使用了未定义类型“QtButtonPropertyBrowserPrivate”1&gt;E:\\QtPropertyBrowserDemo\\QtPropertyBrowserDemo\\qtpropertybrowser\\qtbuttonpropertybrowser.h(47): message : 参见“QtButtonPropertyBrowserPrivate”的声明1&gt;E:\\QtPropertyBrowserDemo\\QtPropertyBrowserDemo\\x64\\Debug\\moc\\moc_qtbuttonpropertybrowser.cpp(96,29): error C2027: 使用了未定义类型“QtButtonPropertyBrowserPrivate”1&gt;E:\\QtPropertyBrowserDemo\\QtPropertyBrowserDemo\\qtpropertybrowser\\qtbuttonpropertybrowser.h(47): message : 参见“QtButtonPropertyBrowserPrivate”的声明 解决办法如下： 在 VS 项目目录内右键对应 moc 文件的头文件，选中属性—Qt Meta-Object Compiler—moc，将 C++ Dynamic Source 值从 “Output File” 改为 “Disable”，重新编译即可。 简言之，这个项的作用是将当前文件中 Qt 类生成的 moc 源文件在编译阶段动态添加到编译器的源码文件列表中。 因为 Private 类的定义在 x.cpp 文件而非 x.h 中，导致编译 moc_x.cpp 时找不到类定义。 项值 Disable 即不将 moc_x.cpp 动态添加到编译器中。这么做是没问题的，因为 QtPropertyBrowser 相关源文件的最后都已显式 include 了对应的 moc.cpp。 具体可参考： https://forum.qt.io/topic/119401/how-to-compile-source-code-of-qtpropertybrowser-by-vs2019-correctly https://www.qt.io/blog/2018/01/24/qt-visual-studio-new-approach-based-msbuild","link":"/VS2019%E7%BC%96%E8%AF%91QtPropertyBrowser%E6%BA%90%E7%A0%81.html"},{"title":"XOPlayer 阶段性总结——使用 Unity 的 VideoPlayer 组件开发全景播放器","text":"项目地址 现在还只是一个简陋的单 Scene 的应用，但已具备基本的视频播放和播放控制能力： 本地视频播放 进度显示、跳转 暂停、恢复、停止 音量调节 视频源类型切换 播放界面如下： 0. 开发、运行环境 VS2019 Community。安装时务必勾选 Visual Studio Tools for Unity。 Unity 2018.4.14f1。运行时修改菜单 Edit-Preferences 的 External Tools 选项页中的 External Script Editor 项值，关联到 VS2019，方能在 VS2019 中正确识别 Unity 工程和源码。 AMD Ryzen7 1700 + GTX 1070 + DDR4 16GB 1. UI 布局一览 模块化的组合功能均以 Prefab 实现（蓝色组件）。Prefab 既提供了一种可复用手段（2次出现的 Slider 以同一 Prefab 实例化），又契合了高内聚，低耦合的设计理念。我的理解是：相比于直接在 Scene 中布局复杂界面，Prefab 是更优选择，能用则用。 注意 VideoPlayer 所在位置，出现在根 Canvas 之外，原因参考 2. VideoPlayer。 2. VideoPlayer 相关以流媒体角度解读 VideoPlayer 的话，VideoPlayer 是一个 demuxers，decoders 和 renders 的组合。注意这里说的渲染，非指渲染到 UI，而是以某种标准格式渲染到内存或显存，从内存/显存渲染到 UI 是用户职责。所以，VideoPlayer 被设计为非 UI 组件，这也是上文提到 VideoPlayer 为什么出现在根 Canvas 之外的原因。 VideoPlayer 提供了多种 Render Mode，适配不同的渲染场景，XOPlayer 只用到了 Render Texture 一种。此纹理须在脚本中动态创建并关联到 VideoPlayer ： 12345678910// make sure: mPlayer.prepareCompleted += onPrepareCompleted.// width &amp; height are available until prepared.private void onPrepareCompleted(VideoPlayer videoPlayer){ mVideoTexture = new RenderTexture((int)mPlayer.width, (int)mPlayer.height, 0, RenderTextureFormat.ARGB32); mPlayer.targetTexture = mVideoTexture; // attaching texture to material // attaching material to some kind of UI controls, e.g., Image, global Skybox. mPlayer.Play();} XOPlayer 支持普通视频和全景视频的播放，两者在渲染到 UI 时大不相同： 普通视频渲染到 Image 组件 全景视频渲染到全局 Skybox 或 Sphere 此逻辑应补充在 RenderTexture 创建后、视频播放前： 123456789101112// attaching texture to material// attaching material to some kind of UI controls, e.g., Image, global Skybox.if (mMode == PlayMode.kNormal) // normal videos{ video2DMaterial.mainTexture = mVideoTexture; normalPlayer.GetComponent&lt;Image&gt;().material = video2DMaterial;}else // panoramic videos{ videoPanoramicMaterial.mainTexture = mVideoTexture; RenderSettings.skybox = videoPanoramicMaterial;} video2DMaterial 是一个预先定义的材质，Shader 采用 “Unlit/Texture” videoPanoramicMaterial 是一个预先定义的材质，Shader 采用 “Skybox/Panoramic” 全景视频的视角又分为 180° 和 360°，可以通过预先定义 2 个不同的材质，Image Type 分别设置为 180 Degrees 和 360 Degrees 实现；也可以只创建一个材质，脚本中动态切换 Image Type : 123456789if (mMode == PlayMode.kPanoramic180) // 180 degrees panoramic videos{ videoPanoramicMaterial.SetFloat(&quot;_ImageType&quot;, 1f); }else // 360 degrees panoramic videos{ videoPanoramicMaterial.SetFloat(&quot;_ImageType&quot;, 0f);} 3. 其它3.1 全景视频的视角旋转 PC 上通过鼠标拖拽驱动视角旋转。天空盒内，只需要将 Main Camera 按其自身 position 沿 X/Y 轴旋转即可： 123456789101112private float rotateSpeed = 2.0f;// commonly, we track Input status in Update loop.void Update(){ if (Input.GetMouseButton(0)) // if left mouse button pressed down { // rotate about x asix transform.RotateAround(transform.position, Vector3.down, rotateSpeed * Input.GetAxis(&quot;Mouse X&quot;)); // rotate about y asix transform.RotateAround(transform.position, transform.right, rotateSpeed * Input.GetAxis(&quot;Mouse Y&quot;)); }} 3.2 鼠标点击任意处显示/隐藏工具栏 因为绝大部分逻辑均在 VideoPlayer 组件的脚本 PlayerManager 内实现，所以第一反应是通过 PlayerManager 实现 IPointerDownHandler, IPointerUpHandler 接口即可。 结论是不可以。上文已经提及，VideoPlayer 不是 UI 组件，没有 Rect Transform，所以它其实是不可能接受鼠标事件的。 所以需要在根 Canvas 上添加脚本，并实现 IPointerDownHandler, IPointerUpHandler ： 1234567891011121314public class PointerHandler : MonoBehaviour, IPointerDownHandler, IPointerUpHandler{ // attaching to PlayerManager script. public PlayerManager playerManager; public void OnPointerDown(PointerEventData eventData) { } public void OnPointerUp(PointerEventData eventData) { playerManager.OnPointerUp(); // we do actions in PlayerManager. }} 3.3 普通视频播放相关 普通视频的调用链是这样的： VideoPlayer——RenderTexture——video2DMaterial——normalPlayer 其中 RenderTexture 是动态创建的，直接导致 normalPlayer.GetComponent&lt;Image&gt;().material 属性不能通过 Inspector 面板关联（指 video2DMaterial——normalPlayer），甚至在 Start() 中赋值也不行。Image 画面不会随播放更新。没有找到相关资料支持，但是我反推的结论是： 123mVideoTexture = new RenderTexture((int)mPlayer.width, (int)mPlayer.height, 0, RenderTextureFormat.ARGB32);video2DMaterial.mainTexture = mVideoTexture;normalPlayer.GetComponent&lt;Image&gt;().material = video2DMaterial; 这三句的调用顺序是不能变动的。即需要先完备材质信息，才能将材质赋值给 Image (或其它 UI 组件 ?)。 停止播放时存在类似问题： 123mPlayer.Stop();// 这句赋空是必须的，否则 Image 变为不可重入，再播放视频时画面不能更新。normalPlayer.GetComponent&lt;Image&gt;().material = null; 结合创建时的三句代码反推，结论是：再次播放时 VideoTexture 是重新创建的，但是 video2DMaterial 不是，所以 normalPlayer 跟踪不到这个间接变化。 得出一个不知道对错的结论：一个调用链上的对象，最好要么全部静态创建，要么全部动态创建。4. Issues 基本功能缺失：快进、快退、循环 180° 全景视频的旋转角度是 360° 进度条 Slider 不能拖动（OnValueChanged 死循环） 全景视频变糊（存疑）","link":"/xoplayer%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93.html"},{"title":"基于 FFmpeg 的视频编辑器开发—踩坑记","text":"折腾了两周，视频编辑器已初具规模： 解封装 解码 快速跳转和精准跳转 格式转换 缩放 重采样 添加文字 添加 srt 字幕 编码 封装 基本满足了当初想做一个 gif 生成器的需求，也是时候回顾下过去两周踩过的坑了。 1. av_seek_frame() 后解码，第一帧的 pts 仍为 seek 前的 ptsav_seek_frame() 是 avformat 模块接口，seek 后信息没有同步到 avcodec 模块。 seek 后马上调用 avcodec_flush_buffers() 即可。 2. 解码后视频帧 pts 为 AV_NOPTS_VALUEAV_NOPTS_VALUE 是一个极大的负数值的宏定义。pts 字段值是它时，说明此视频格式不支持或此视频未设置 pts。应把 pts 值设为同 pkt_dts 值： 1234// frame is a decoded AVFrameif (frame-&gt;pts == AV_NOPTS_VALUE) { frame-&gt;pts = frame-&gt;pkt_dts;} 3. av_read_frame() 后 调用 av_packet_unref() 释放 AVPacket 对象av_read_frame() 会将传入的 AVPacket 对象变为引用计数形态，或在传入 AVPacket 对象已经是引用计数形态时将计数 + 1，应用层负责在合适时机调用 av_packet_unref 将计数 - 1。否则 AVPacket 对象内缓存区将永远不会释放，导致内存泄漏。 4. 将滤镜图输出到文本文件进行调试分析这其实是个官方提供的命令行工具来的，叫 graph2dot 。只需将其中 print_digraph() 函数定义复制到自己的工程内，即可打印出如下图所示的滤镜图内的关系链： 结合 print_digraph() 源码和生成的文本描述，对 filter graph 的内部逻辑也能略窥一二。 5. 使音频滤镜吐出固定尺寸（采样数）许多音频编码器如 AAC，要求传递给 avcodec_send_frame() 的 AVFrame 对象包含固定尺寸（采样数）的音频数据，否则返回值将会是 AVERROR(EINVAL)。没必要自己缓存音频数据至指定尺寸再发送给编码器，可以直接调用 av_buffersink_set_frame_size() 接口，指示 sink 滤镜总是吐出指定尺寸的帧数据。 1234// 在 avfilter_graph_config() 后调用一次即可if (!(audioCodecContext-&gt;codec-&gt;capabilities &amp; AV_CODEC_CAP_VARIABLE_FRAME_SIZE)) { av_buffersink_set_frame_size(sinkContext, audioCodecContext-&gt;frame_size);} 6. 封装时应重新计算 pts编码时，应以写入的实际帧率/采样率设置编码器的 time_base 字段，同时将输入帧的 pts 从零计算： 123456789101112// for video encodervideoCodecContext-&gt;time_base = { 1, fps };// ...// int vPts = 0;videoFrame-&gt;pts = vPts++;// for audio encoderaudioCodecContext-&gt;time_base = { 1, sampleRate };// ...// int aPts = 0;audioFrame-&gt;pts = aPts;aPts += audioFrame-&gt;nb_samples; 最后，在编码后写入前，应将 AVPacket 时间戳转换为相应流的 time_base 单位： 1av_packet_rescale_ts(&amp;packet, codecContext-&gt;time_base, stream-&gt;time_base); 7. 解封装到文件尾（EOF）时，末尾若干帧丢失av_read_frame() 返回 AVERROR_EOF 时，不应该直接结束后续操作。编解码器、滤镜均不是一进一出，所以很有可能在 av_read_frame() 返回 AVERROR_EOF 时，编解码器、滤镜中仍存在待处理的帧数据。 对解码器，以一个空 AVPacket 指针传入 avcodec_send_packet() 可以起到 flush 对应解码器的作用； 同理，对编码器应以一个空 AVFrame 指针为参数调用 avcodec_send_frame()； 滤镜的 flush 有两种方式： 以空 AVFrame 指针调用 av_buffersrc_add_frame_flags() 不需要调用 av_buffersrc_add_frame() 或 av_buffersrc_add_frame_flags()，直接以 AV_BUFFERSINK_FLAG_NO_REQUEST 参数调用 av_buffersink_get_frame_flags()","link":"/%E5%9F%BA%E4%BA%8EFFmpeg%E7%9A%84%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91%E5%99%A8%E5%BC%80%E5%8F%91%E2%80%94%E8%B8%A9%E5%9D%91%E8%AE%B0.html"},{"title":"用 FFmpeg 制作 GIF","text":"ffmpeg -y -ss 1:33:28 -t 22 -i input.mp4 -vf &quot;scale=iw*0.3:ih*0.3,drawtext=x=(w-text_w)/2:y=h-40:fontsize=30:fontcolor=white:fontfile=C\\\\:/Windows/Fonts/STXINWEI.TTF:text='这是文字'&quot; -r 15 output.gif 从 input.mp4 的 1:33:28 处开始截取 22 秒；输出尺寸缩小到输入的 0.3 倍；在输出图的中下部分添加文字，文字样式为白色、30像素、新魏；把输出 gif 图的帧率设为 15。 note: 1) 多个 filter 效果以,拼接，且以出现的先后顺序逐个应用到输出上。所以，drawtext 里用到的w、h是缩放后的尺寸；2) 注意fontfile值的表现方式。","link":"/%E7%94%A8FFmpeg%E5%88%B6%E4%BD%9CGIF.html"},{"title":"用 FFmpeg 下载视频","text":"ffmpeg -i &quot;https://us.sinaimg.cn/0013JKJ9jx07aaEhInRC0104010094440k01.mp4?label=mp4_hd&amp;Expires=1579700718&amp;ssig=wkpUBmK%2B9Y&amp;KID=unistore,video&quot; d:\\output.mp4 从 -i 标识的 url 下载视频保存到 output.mp4。 note: url 包含特殊字符(空格、&amp; 等)时，必须用双引号包含 url 才能正确解析；输出路径同理。","link":"/%E7%94%A8FFmpeg%E4%B8%8B%E8%BD%BD%E8%A7%86%E9%A2%91.html"},{"title":"用 FFmpeg 剪辑视频片段","text":"ffmpeg -ss 300 -t 600 -i c:\\input.mp4 c:\\output.mp4 从 input.mp4 的 300s 处开始剪辑，时长 600s，保存到 output.mp4。 note: -ss -t 参数都可应用于输入或输出(ffmpeg -i c:\\input.mp4 -ss 300 -t 600 c:\\output.mp4)上，区别在于：-ss 应用于输入时有性能优势，会先跳转到时间点再进行解码；用于输出时会逐帧解码并丢弃时间点前的所有帧。","link":"/%E7%94%A8FFmpeg%E5%89%AA%E8%BE%91%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5.html"},{"title":"用 FFmpeg 实现画中画效果","text":"ffmpeg -i input1.mp4 -stream_loop -1 -i input2.gif -filter_complex overlay=shortest=1 output.mp4 把 gif 放在 mp4 的左上角。-stream_loop -1 指示 gif 不断循环，否则 gif 会停留在最后一帧；shortest=1 指示 overlay 操作在任意一个输入读取到文件尾时结束。 note: stream_loop 是输入项参数，所以要放在要应用的输入的后面。","link":"/%E7%94%A8FFmpeg%E5%AE%9E%E7%8E%B0%E7%94%BB%E4%B8%AD%E7%94%BB%E6%95%88%E6%9E%9C.html"},{"title":"用 FFmpeg 制作视频滚动效果","text":"ffmpeg -i input.mp4 -filter_complex &quot;split[o1][o2];[o1]select=between(n\\,0\\,100),scroll=h=0.01[oo1];[o2]select=gt(n\\,100),setpts=N/FR/TB[oo2];[oo1][oo2]concat=n=2:v=1&quot; -map 0:a output.mp4 把输入 mp4 前 100 帧做成由右到左的滚动效果。 scroll 的速率是每帧行进的视频宽/高的百分比。0.01 即每帧位移 0.01 * width 像素，所以整个滚动效果需要 100 帧。 note: 1) 第二段 select，必须从 0 重新计算 pts，否则拼接时候效果不对；2) 此法只适合短小视频操作，因为所有滤镜效果都缓存在内存；3) scroll 是 FFmpeg4.3 版本新加入的滤镜效果，用前确认版本。","link":"/%E7%94%A8FFmpeg%E5%88%B6%E4%BD%9C%E8%A7%86%E9%A2%91%E6%BB%9A%E5%8A%A8%E6%95%88%E6%9E%9C.html"},{"title":"用FFmpeg打印音频波形图","text":"1ffmpeg -i input.mp4 -filter_complex showwavespic=s=1800x600:split_channels=1 output.jpg 把 input.mp4 里的默认音频流的采样波形图打印到 output.jpg 。s=1800x600 指定输出图片尺寸；split_channels=1 可以把多个声道分别打印一行。","link":"/%E7%94%A8FFmpeg%E6%89%93%E5%8D%B0%E9%9F%B3%E9%A2%91%E6%B3%A2%E5%BD%A2%E5%9B%BE.html"},{"title":"用 FFmpeg 录屏","text":"1ffmpeg -f gdigrab -video_size 1280x720 -offset_x 120 -offset_y 140 -show_region 1 -i desktop -c:v libx264 -preset:v ultrafast -t 10 output.mp4 从屏幕 (120,140) 像素起抓取 1280x720 尺寸的屏幕画面，时长 10 秒。-show_region 1 会在屏幕上显示一个抓取范围的矩形提示框。 note: gdigrab 只适用 Windows 系统，且只抓取画面没有声音。 FFmpeg Wiki 有关于录屏更全面的介绍。","link":"/%E7%94%A8FFmpeg%E5%BD%95%E5%B1%8F.html"},{"title":"用 FFmpeg 给 mp4 添加封面","text":"1ffmpeg -i input.mp4 -i cover.png -map 0 -map 1 -c copy -disposition:v:1 attached_pic output.mp4 将图片 cover.png 做为 input.mp4 的封面图。 关键是 -disposition:v:1 attached_pic 这句，即把 map 后的第二个视频流（ cover.png ）做为 output.mp4 的封面。","link":"/%E7%94%A8FFmpeg%E7%BB%99mp4%E6%B7%BB%E5%8A%A0%E5%B0%81%E9%9D%A2.html"},{"title":"西安行有感","text":"西安一行，让我对地方美食文化的传播有了一些自己的思考。 比如凉皮、肉夹馍，味道跟我在广州吃到的一些无异，因为本身就是大众口味，制作也简单，应该是被原样继承了下来的。 一个反例是羊杂汤。在我们老家，羊杂汤、羊肉汤在制作流程上是一样的，无非是内容差别，所以最后呈现的味道，也大体是一样的。但是在西安，羊杂汤跟羊肉汤（水盆羊肉），简直是天差地别的两个东西。所谓正宗羊杂汤的味道，怕是非少数人不可承受吧？ 但是我不知道这两个东西是不是源自一处，也不知道我们老家的羊汤是不是源自西安。也许只是我道听途说惯了，给自己错误的植入了“天下羊汤出西安”的记忆？所以这句话就当做是免责声明了吧。 总之，经历了这么多次国内旅行，我已经对正宗地道的地方菜有了免疫。在家门口吃到的，都是经过演绎变化，做成了适合你口味的罢了。好吃却在别的地方做不出来、发展不起来的菜我反而没见过。所以不必太计较于是不是正宗，更没必要对发源地心心念念。 话虽如此，对西安的失望却尤其难以抚慰，那是我念叨了小半辈子的地方……","link":"/%E8%A5%BF%E5%AE%89%E8%A1%8C%E6%9C%89%E6%84%9F.html"},{"title":"通过 HomeAssistant 查询市面上开源&#x2F;开放协议的智能家居产品小攻略","text":"HomeAssistant 是一个开源的、Python 实现的、可本地部署 的智能家居自动化服务。至今已集成了上千种智能家电、开放平台，比如 Yeelight、Opple（欧普）、Google Assistant 等等等等。 可以说，HomeAssistant 基本满足了我对智能家居中控系统的要求，尤其我一直强调的安全性问题。市面上绝大多数智能家居平台都是远程式的、不可部署的，直接引入了两大安全隐患： 数据被服务提供商有意或无意地泄漏； 数据在远程传输过程中被截取、篡改。 所以一个本地化的服务一直是我的追求。在我知道 HomeAssistant 存在之前就一直琢磨怎么利用树莓派搭建一个中控系统，无奈的是各种产品的通信协议都像一个黑盒子，无从入手。直接用 HomeAssistant 又太重。前端显示我不需要，上千种的组件我肯定也用不到，其自动化的配置方式也有学习成本。所以，我最需要的只是 HomeAssistant 的源码。 HomeAssistant 支持的产品均以组件的形式存放在 https://github.com/home-assistant/core/tree/dev/homeassistant/components 目录下，定位到某一品牌后，在其控制源码里就可以一览其实现方式。 以格力（gree）为例，在 https://github.com/home-assistant/core/tree/dev/homeassistant/components 页面搜索 gree，跳转到 ./gree 目录，点开 climate.py，有如下字样： 1234567from greeclimate.device import ( FanSpeed, HorizontalSwing, Mode, TemperatureUnits, VerticalSwing,) 像是厂商提供的一个包，去到 https://pypi.org/ 搜索包名，就找到了。文档描述其 should work for any device that also works with the Gree+ app，厉害了。 同样的方式，我也定位到了 Yeelight、Opple 这两个国内大厂的 python 包。 NGO 万岁，标准万岁，但不包括 BCI。","link":"/%E9%80%9A%E8%BF%87HomeAssistant%E6%9F%A5%E8%AF%A2%E5%B8%82%E9%9D%A2%E4%B8%8A%E5%BC%80%E6%BA%90-%E5%BC%80%E6%94%BE%E5%8D%8F%E8%AE%AE%E7%9A%84%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E4%BA%A7%E5%93%81%E5%B0%8F%E6%94%BB%E7%95%A5.html"},{"title":"智能家居产品调研备忘","text":"协议调研中发现，小米开放了一套自研的控制协议 https://github.com/OpenMiHome/mihome-binary-protocol 。对应的 python 实现在这里 https://github.com/rytilahti/python-miio 。下文提到的云仪开窗器就是采用的 miio 。miio 涉及一个 token，看了下源码，token 大概是端到端的数据加密密钥的作用，https://github.com/al-one/hass-xiaomi-miot#obtain-miio-token 这里有获取 token 方法。 灯具yeelight。控制协议完全开放，https://www.yeelight.com/download/Yeelight_Inter-Operation_Spec.pdf，有可直接使用的 python 包， https://pypi.org/project/yeelight/ 。大概看了下是参照 spec 实现的。 opple。有一个 python 包可用，https://pypi.org/project/pyoppleio/ 。但是有几年没有更新了，有一个 opened issue 提到已不能正常工作。以 homebridge opple 为关键字 google，对应插件也是年久失修状态。 飞利浦 hue。hue 是一个系列/平台，有蓝牙/网关两个子产品分类。蓝牙是直连终端，需自己 hack 其 app 获取协议；网关到终端是 zigbee 协议的，不是简单的透传，不知道咋 hack。 所以选购优先级应该是 yeelight、opple、其它。 窗帘窗帘基本上是绿米一家独大，但是绿米应该是优先级最低的选择。多数产品要么需要网关、要么走云端，很少两点一线的产品。且协议不开放，需要自己 hack。但是其到网关的协议是开放的，https://github.com/home-assistant/core/tree/dev/homeassistant/components/xiaomi_aqara 。运气好如果网关到窗帘是透传的话，倒是没问题。 其它都差不多吧。选购标准还是必须是端到端 wifi 控制的。很多所谓厂家直销的小品牌，可以尝试跟他们沟通是否可以拿到传输协议。 杜亚（dooya）。github 搜 dooya 有几个其它语言实现的 dooya 无线（433MHz）控制协议，没找到 wifi 版本。 找到一个论坛定制窗帘。https://bbs.hassbian.com/thread-4814-1-1.html，源码以 hass 插件形式开放。 米家协议窗帘。https://bbs.hassbian.com/thread-11894-1-2.html，https://github.com/tiandeyu/mijia_curtain/tree/master/custom_components 。看论坛讨论大概是所有遵循 MIoT-Spec 的窗帘设备都可以控制？这样的话选购范围就大了，基本上所有标识支持 小米APP/小爱音箱 的窗帘都可以用。 开窗器开窗器就完全是一个散乱的市场了，全是厂家定制那种。也可以尝试跟他们要协议。 云仪。https://github.com/dominic2708/yunyi_windows 。使用的是小米的协议方案。 空调格力。有 python 包，https://pypi.org/project/greeclimate/ 。homebridge 也有格力的插件，https://www.npmjs.com/package/homebridge-gree-heatercooler-v2 。都是活跃更新的，问题应该不大。 美的。有一个 HA 插件源码包，https://github.com/mac-zhou/midea-ac-py 。","link":"/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E4%BA%A7%E5%93%81%E8%B0%83%E7%A0%94%E5%A4%87%E5%BF%98.html"},{"title":"准备训练自己的智能家居语音助手","text":"Mozilla 实现的 DeepSpeech 语音识别引擎可以离线使用、可以运行在树莓派 4 上，简直就是给我准备的。 它附赠了一个训练好的普通话模型，我暂时没有试用。但训练自己专属的语音助手明显更带感，也更能贴近自己的说话习惯。 比如可以给它起一个很拉风的名字——贾维斯，还可以给它取几个昵称，小贾、老贾、贾爷。每当我喊出其中任何一个名字时，它都应该能识别到我是在和它说话。 市面上的语音助手，多以交互式对话完成指令。比如你问它天气预报： 1234我：小爱同学小爱同学：在我：今天几度小爱同学：今天广州 blah blah blah 这大概是对语音助手寄托了过多的私欲吧，幻想它真得会像一个朋友甚至一个红粉知己一样存在、同你交流。这方面我就理智得多了，我只希望它能完成我的指令即可。所以我会把上述对话简化成一个问答： 12我：贾维斯今天几度贾维斯：今天广州 blah blah blah 这更符合我的个性，同时也更贴合智能家居这样一个功利性极强的场景。 同时也会节省很多训练时间。依自己的说话习惯和智能家居这种词句范围有限的使用场景，训练语料应该不需要太多。","link":"/%E5%87%86%E5%A4%87%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E8%AF%AD%E9%9F%B3%E5%8A%A9%E6%89%8B.html"},{"title":"Ubuntu上安装并训练DeepSpeech","text":"全程参照官方文档 ，仅少许浅坑。这篇文章算是摘要 + 翻译 + 注解吧。 前提条件 Python 3.6 严格遵守，实测 3.8 版本会有诸多错误。 Mac 或 Linux 环境 建议避过 Mac，缺失的依赖比 Linux 多很多，问题也多，我是半途放弃 Mac 转到 Ubuntu 的。可以通过启用 Windows 10 的 WSL 功能创建 Ubuntu 环境，可以使用所有的硬件能力。 CUDA 10.0 / CuDNN v7.6 per Dockerfile. 这条为非必须，因为我是新手，直接用 CPU 训练的。 下载 DeepSpeech 源码1git clone --branch v0.9.3 https://github.com/mozilla/DeepSpeech 创建 Python 虚拟环境使用默认的 venv 方式创建： 1python3 -m venv ~/tmp/deepspeech-train-venv/ 激活虚拟环境source ~/tmp/deepspeech-train-venv/bin/activate 然后下边的所有操作都应该在此虚拟环境中进行。 安装 DeepSpeech 依赖123cd DeepSpeechpip3 install --upgrade pip==20.2.2 wheel==0.34.2 setuptools==49.6.0pip3 install --upgrade -e . 后续如果有更新 DeepSpeech 源码，需要再次执行上边最后一句 pip3 install 命令，确保依赖对得上号。 这个过程中会遇到一个错误： 12345ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.tensorflow 1.15.4 requires numpy&lt;1.19.0,&gt;=1.16.0, but you'll have numpy 1.19.5 which is incompatible. 依照错误提示卸载 numpy 再装一个指定版本的包即可： 123pip3 uninstall numpypip3 install numpy==1.16.0pip3 install tensorflow==1.15.4 然后安装一个 webrtcvad 的依赖包： 1sudo apt-get install python3-dev 执行 DeepSpeech 预置脚本完成一次简单训练跳转到 DeepSpeech 根目录 后执行以下脚本： 1./bin/run-ldc93s1.sh 上边脚本会下载语料，处理成 DeepSpeech 可识别格式，然后进行训练。下载的语料和处理后的数据都保存在 DeepSpeech/data/ldc93s1/ 目录下。 DeepSpeech 要求必须用 16 bit 位深、单声道的音频进行训练、识别；且训练与识别所用的音频的采样率也必须相同。 几分钟就训练完了。但是这个脚本没有将训练后的模型保存成文件，可以打开脚本，给在最后边执行的 DeepSpeech.py 脚本添加一个参数： 12345678910python -u DeepSpeech.py --noshow_progressbar \\ --train_files data/ldc93s1/ldc93s1.csv \\ --test_files data/ldc93s1/ldc93s1.csv \\ --train_batch_size 1 \\ --test_batch_size 1 \\ --n_hidden 100 \\ --epochs 200 \\ --checkpoint_dir &quot;$checkpoint_dir&quot; \\ --export_dir /mnt/g/ \\ # 这行就是新添加的，可以将模型保存到指定目录 &quot;$@&quot; 在 /mnt/g/ 目录下会生成一个 output_graph.pb 模型文件。 使用 Mozilla 提供的数据集Mozilla 收集了很多语言的数据集，包括普通话 。下载下来的是一个 tar 压缩包，解压后得到一串 clips/*.mp3 文件和若干 tsv 文件。 执行 bin/import_cv2.py 脚本： 1bin/import_cv2.py /path/to/extracted/language/archive 在 clips 目录下创建了 mp3 对应的 wav 文件，和若干 csv 文件。这些就是 DeepSpeech 可识别的输入了，参照上边 run-ldc93s1.sh 中 DeepSpeech.py 脚本的用法，就可以用这些数据集训练了。","link":"/Ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85%E5%B9%B6%E8%AE%AD%E7%BB%83DeepSpeech.html"},{"title":"自定义语音助手语料设计","text":"旺财 客厅 卧室 书房 窗帘 窗户 空调 灯 开 关 开灯 关灯 开窗帘 关窗帘 开窗户 关窗户 开空调 关空调 旺财开灯 旺财开客厅灯 旺财把客厅灯打开 旺财关窗帘 旺财关客厅窗帘 旺财把客厅窗帘关上 旺财开窗帘 旺财开客厅窗帘 旺财把客厅窗帘打开 旺财开空调，二十五度 旺财开窗户 旺财开客厅窗户 旺财把客厅窗户打开 旺财关窗户 旺财关客厅窗户 旺财把客厅窗户关上 旺财开卧室灯 旺财把卧室灯打开 旺财关卧室窗帘 旺财把卧室窗帘关上 旺财开卧室窗帘 旺财把卧室窗帘打开 旺财开卧室窗户 旺财把卧室窗户打开 旺财关卧室窗户 旺财把卧室窗户关上 旺财开书房灯 旺财把书房灯打开 旺财关书房窗帘 旺财把书房窗帘关上 旺财开书房窗帘 旺财把书房窗帘打开 旺财开书房窗户 旺财把书房窗户打开 旺财关书房窗户 旺财把书房窗户关上","link":"/%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%AD%E9%9F%B3%E5%8A%A9%E6%89%8B%E8%AF%AD%E6%96%99%E8%AE%BE%E8%AE%A1.html"},{"title":"DeepSpeech 调参备忘","text":"最优解迄今为止训练效果最好的一组参数集是： 1python DeepSpeech.py --train_files /mnt/g/myowncorpus/dst.csv --dev_files /mnt/g/myowncorpus/dst.csv --export_dir /mnt/g/myowncorpus/ --checkpoint_dir /mnt/g/myowncorpus/checkpoints128 --alphabet_config_path /mnt/g/myowncorpus/alphabet.txt --dropout_rate 0.3 --early_stop True --n_hidden 128 --export_file_name hidden128 --export_author_id hidden128 --epochs 200 --export_model_version 0.0.3 --lm_alpha 0.6940122363709647 --lm_beta 4.777924224113021 --export_language cmn-Hans-CN --reduce_lr_on_plateau False --es_epochs 800 --test_files /mnt/g/myowncorpus/dst.csv --learning_rate 0.001 --beam_width 100 checkpoint_dir checkpoint 路径。checkpoint 是一种增量训练机制，允许保存之前最近的、最优的若干次训练结果，下一次加载后继续训练。 n_hidden 隐含层数量。对隐含层解释地比较通俗易懂的是这篇文章 。直接影响有 3 个： 1) 训练速度，越小越快 2) 模型文件尺寸，值越小文件尺寸越小 3) 训练效果，无绝对好坏，取决于训练集。我是从默认的 2048 一直减半到现在的 128 测试出来的最优值。 调整 hidden 必须从头训练，不能沿用之前的 checkpoint 。 alphabet_config_path 字母表。中文训练即训练集中出现的所有汉字的集合。 dropout_rate 随机丢掉隐含层的比例。 epochs 训练次数。不是越多越好，反而有可能变坏。像我现在数据集只有 113 条，最佳的训练次数是 200 次。 lm_alpha lm_beta 语言模型参数，取值是官方推荐值。 问题总是有若干条目识别特别差，最典型的是这个： 语音：旺财关客厅窗帘 文本：旺财把客厅窗帘 语音：旺财开书房灯 文本：旺财把书房灯 调试了各种参数还是不能解决。怀疑跟数据集太小、相关录音录制得不清楚有关系。 参考 隐藏层：https://blog.csdn.net/JH0lmes/article/details/82777269 CNN 入门讲解：什么是dropout?：https://zhuanlan.zhihu.com/p/77609689 https://github.com/mozilla/DeepSpeech/releases/tag/v0.9.3","link":"/DeepSpeech%E8%B0%83%E5%8F%82%E5%A4%87%E5%BF%98.html"},{"title":"智能家居终端控制系统设计","text":"手里有闲置的 Surface 3 平板、Android 平板、Android 手机各一台，正好可以把它们分散固定在常驻区域的墙面上，一可以用作多媒体终端（这个功能是现成的），二就是可以用作智能家居的控制终端，和我的树莓派（中控系统）进行双工通信。 这是我设想的语音助手外的另一个重要的控制路径。比如，你已经在终端附近时，或超出拾音范围时，或者你恰巧在一直嗡嗡响的洗衣机周边时，通过屏幕终端下发命令应该是更优解。 3 个固定的终端大概是分别放在厨房、客厅、阳台。树莓派作为服务端，允许多个客户端的接入，推送设备状态信息到所有终端。最终的网络连接很可能不是图里这种 all to all 的结果，因为我很可能有两段路由。 终端的软件实现，大致确定以 web 页面形式实现，否则分别开发原生应用的代价太大了。前端框架用 React，小巧、组件化实现，上手简单；搭配 Spectre CSS 库，开发一个看起来差不多的 UI 应该是够了。 网络传输自然是 Websocket 了，树莓派是服务端，各个终端是客户端。因为是局域网传输，数据量也不大，所以应该直接传输 json 数据。具体的数据模型待定，跟最后选定的家电的能力强关联，现在写得太细也没什么意义。 待续…","link":"/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E7%BB%88%E7%AB%AF%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1.html"},{"title":"通过人体识别实现智能家居控制","text":"市面上普遍使用的人体检测方案是红外传感器，缺点很多且显得特别弱智： 只能检测运动物体； 完全不能区分人或其它活动物体； 不能区分人体是进入或离开某一区域。 设想一个最简单普遍的生活场景：当我晚上 7 点步入餐厅时，餐厅灯光应随之亮起；同理，当识别到餐厅无人时，应自动将餐厅灯关闭。通过人体传感器是死活做不到这点的。 转换思路，视餐厅为一平面图，上述问题即可抽象成 “如何检测一张图片内是否有人存在”，解决方案就显而易见了——目标检测。 识别算法YOLOv5 没能部署到树莓派上（官方 32 位系统），也没深究。转头发现了 Paddle-Lite ，专为移动端、嵌入式硬件开发的深度学习推理框架。Paddle 提供了超多预训练模型 ，我选取了其中尺寸最小的MobileNetV3 Small ，实测在树莓派4 上每次推理仅需 &lt; 200 ms。 摄像头这甚至是个比识别率还严重的问题。基本上所有的 USB 摄像头都不具备夜视能力，用这种摄像头正应了达文西手电筒的梗。无奈买了一个网络摄像头，因为需通过 RTSP 协议获取视频流，缺点同样明显： 占用宽带； 高延时； 需要解码。 所以我现在设定的采集帧率仅为 2 帧/秒，尽量降低树莓派的功耗。 问题 识别速度。 暂时没找到只一个人体分类的模型，这是推理速度的一个瓶颈。终极方案大概只能是自己训练模型。 识别正确率。 一个是模型没有按场景训练过；一个是单一摄像头角度固定，需要一个摄像头阵列从不同角度采集后综合得出是否有人的最终结论。这不管是从经济角度还是技术角度，都是个大工程。 视频采集延时。 得继续找一个更靠谱的摄像头，至少 3 米的夜视能力是基本要求，USB 有线是最好的了。现有的网络摄像头方案，大概可以通过把摄像头和树莓派都有线连接到路由器降低延时。 源码(https://github.com/LiangZuoting/PersonMonitor) 。PersonMonitor 把识别结果推送到 SmartHome Web API，SmartHome 控制灯具做出响应。 后续摄像头问题已完美解决。 在 1688 上淘到了这个 USB 接口的夜视摄像头。 深究之后发现，摄像头延时问题除 RTSP 协议原因外，还有一个很重要的点是 OpenCV 会默认缓存 4-5 帧图像。解决方案就是把摄像头采集放在单独线程里进行，且采集速度应明显高于识别速度。比如我的设定是每 200ms 识别一次，每 100ms 采集一次，利用 100ms 的时间差尽可能丢掉前边的缓存帧，只保留最新的一帧。这样，识别也是基于最新帧了。","link":"/%E9%80%9A%E8%BF%87%E4%BA%BA%E4%BD%93%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E6%8E%A7%E5%88%B6.html"},{"title":"Sanic 还不错","text":"简单横向比较了 Sanic、Flask、FastAPI 之后，决定以后还是优先学 Sanic 吧。 Sanic 在同类评测里名列前茅。异步架构有先天的性能优势，Flask 甚至才认识到这点，好像又显得不那么重视。最新发布的 2.x 版本里的异步支持形同鸡肋，不支持 ASGI，不支持后台任务，Websocket 也同样不能支持。 当红炸子鸡 FastAPI 性能同样表现优异，写代码还送文档，着实吸睛，但是那 600 多个 issue 令我“望而生畏”。 模版支持对我反倒是减分项。我是前后端分离的忠实信徒，Sanic 这种一刀切的做法正和我胃口，有事抛 request 过来，合理的性能损耗完全是可以接受的。 Sanic 不只是开发框架，同时提供了产品级的 ASGI 服务器，对新人比较友好，部署上也能省点事，这是 FastAPI 也没能做到的一点。 一些细节上的做法 Sanic 也同样很合我意。比如 request 对象必须做为 view 函数第一个参数进行显式传递。相比 Flask Thread Local 的实现方案，这个额外的参数更像是设计合理的必然结果，因为借此你可以直观地获知 request 对象的生命周期。强制要求 view 返回 HTTPResponse 对象也同具此意。 这几天翻阅了不少 Python Web 框架的文档，最直观感受就是太卷了。如果分别用这些框架实现同样一个功能，查重率应该不会低于 80% 吧？","link":"/Sanic%E8%BF%98%E4%B8%8D%E9%94%99.html"},{"title":"外链 Jellyfin 视频播放页地址","text":"我自己的检索服务本人珍藏的视频比较多，有 TB 级别，所以之前我通过人工给视频文件打 tag 的方式，自创了一套文件检索系统。比如： 1snis243_吉川爱美_单体_好身材_漂亮.mp4 这样一个文件名，约定以下划线分隔 tag，且第一个 tag 总是视频标题。 生成检索数据库后，我用 Sanic + React 实现了 B/S 结构的检索服务。 首页展示所有的 tag： 检索结果页列出所有的匹配视频： 因为是在本地播放，Windows 上有比较万能的播放器，所以当时就没做转码服务，直接用默认浏览器播放选中视频即可。 这套系统很符合我个人阅片习惯，可以依着自己当时的心情按 tag 选片。 把 Jellyfin 和检索服务结合起来6.18 入手了群晖 NAS，支持 Docker。赶紧把电脑上珍藏了好久的视频资源拷贝到了 NAS 上一份，也学别人在 Docker 上用 Jellyfin 搭建了个视频播放服务。 但是 Jellyfin 不识别我的 tag 约定，我的检索系统又没有转码播放服务，两边都用得不顺手。就想到把两边的功能结合一下，用检索系统选片后，跳转到 Jellyfin 的视频详细页面，利用 Jellyfin 播放视频。 Jellyfin 视频页面的地址格式如下： 1http://192.168.3.34:8096/web/index.html#!/details?serverId=xxxxxx&amp;id=xxxxxxxx serverId id 都是一个 32 字符的字符串，serverId 值可以视为一个常量，硬编码到代码里即可。难的是 id 参数值的规则。 最终通过翻阅 Jellyfin 源码，发现所有媒体信息都存储在 library.db 的 sqlite 数据库中，在一个叫 TypedbaseItems 的表里，存储了视频文件路径到 PresentationUniqueKey 的映射关系，这个 PresentationUniqueKey 就是 id。 直接把我生成检索数据库的 generator.py 改掉，无需再遍历文件系统，通过读取 library.db 就满足需求了。 至此，当我在自己的检索服务中选中某项后，就可以跳转到视频对应的 Jellyfin 页面了。 小小后记最开始盲猜 serverId id 都是 md5 或其它摘要算法的产出，在 Jellyfin 源码里一顿全局搜索，浪费了一些时间。后来才想到可以直接查阅它的数据库，”暴破”文件到 id 的映射。事后才发现，id 是 Guid 来的，根本不是通过对文件信息摘要所得。","link":"/%E5%A4%96%E9%93%BEJellyfin%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE%E9%A1%B5%E5%9C%B0%E5%9D%80.html"},{"title":"视频编辑器 ffcutter","text":"ffcutter 初版写完有段日子了。本想着把功能再丰富下， UI 规范后再开源，无奈此后一连数月都忙于新家的智能家居系统的设计开发，ffcutter 就淡忘了。就以现状开源了吧，献给有缘人 。 项目地址 开发环境 C++17 Qt 5.15 FFmpeg 4.3.1 Visual Studio Community 2019 代码结构 如上图，核心能力以 fccore 动态链接库形式提供，FCService 为接口类。对 FFmpeg 各个功能模块再做一层类封装，简化操作步骤，方便复用。 类名均以 FC 为前缀，取自 Fast forword Cutter。 功能介绍快速跳转跳转到指定时间戳后的第一个关键帧。 所谓快速，即只解封装，不解码。对 h264 等帧间编码格式，只有关键帧是内容自足的，B 帧、P 帧的图像内容均依赖相邻帧共同还原。 具体技术细节可参考 FFmpeg AVSEEK_FLAG_BACKWARD 文档。 精确跳转跳转到不晚于指定时间戳后的第一帧。 跳转耗时同 GOP 大小有很大关系。如果恰好需要跳转到某一关键帧之后的一帧，就需要解码 GOP - 1 帧。 通过预览图快速确定视频截取的起止时间左键双击预览图，使用预览图对应帧的时间戳填充起始时间编辑框；右键双击，快速填充结束时间编辑框。 如要截取 jpg 等静态图，左键双击要截取的帧，右键双击下一帧。 文字文字以 FFmpeg drawtext Filter 实现，支持 文本表达式 以显示动态内容，但为了简化使用方式，只支持全视频添加固定文字，不支持指定时间段。 文字支持自选字体，作者存放于 repo 中的几种字体，仅为测试之用，请在使用前确认其授权形式。 字幕以 FFmpeg subtitles Filter 实现，支持 srt ass 等格式。除支持复杂的时间戳规则外，字幕同文字另一个不同的地方在于，字幕是以流（stream）的形式存储于容器文件中的，而非修改视频帧，将文字绘制于上。 自定义滤镜理论上支持所有其它 FFmpeg Filter，语法同 ffmpeg.exe，ffcutter 会将其拼接到上边所列 Filter 之后。 不足之处UI 简陋敝人区区一草莽大汉，于此精巧之工，实无能为力也~ 产品思路感人没有时间线的概念，不能预览全视频；没有视频回放；没有基于音频的剪辑。 多段剪辑、拼接一次只能剪辑一个片段。理想的情况下，应支持多段视频剪辑，然后内存中直接拼接成一个视频。","link":"/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91%E5%99%A8ffcutter.html"},{"title":"C++17 详解 2","text":"本文为 《C++17 in detail》 一书的中文渣中渣译文，不足之处还望指正。 1. 修复和弃用C++17 标准现在有超过 1600 页，比 C++14 多出了 200 多页。幸运的是，语言规范在一些地方得到了清理，一些老旧的或潜在的坏特性随之被清除。 本章你将学到： 从语言中删除了什么，比如：register 关键字，auto_ptr 和 std::random_shuffle。 已弃用并会在未来版本被删除的内容。 修复内容，尤其是对“花括号初始化”（brace initialisation）的自动类型推导。 1.1 删除内容每次 C++ 版本迭代，其中一个核心概念就是和旧版本的兼容性。我们期待语言中出现新东西，同时我们的老工程应该仍然能编译通过。尽管如此，有时候这也是一个删除极少用到或错误部分的机会。 译注：老外说话有时候就是别别扭扭，不够直接。上边这段话大概就是“向下兼容当然是首要的，但有时候不得不删除一些很少用到的或者是错误的东西”的意思。 本节简要说明从标准中删除的内容。 1.1.1 删除 auto_ptr可能是最好的消息了！ C++98 加入 auto_ptr，做为对原生指针支持基本的 RAII 特性的手段。但是，由于当时语言中缺少移动语义，auto_ptr 很容易被误用从而导致运行时错误。 这里是一个 auto_ptr 可能导致崩溃的例子： 123456789101112// Chpater Fixes And Deprecation/auto_ptrCrash.cppvoid doSomething(std::auto_ptr&lt;int&gt; myPtr){ *myPtr = 11;}void AutoPtrTest() { std::auto_ptr&lt;int&gt; myTest(new int(10)); doSomething(myTest); *myTest = 12;} doSomething() 通过传值参数拿到指针，但是 auto_ptr 不是共享型指针（shared pointer），所以它拿到了被管理对象的唯一所有权。当函数执行完毕，pyPtr 走完生命周期，指针对象被删除。 在 AutoPtrTest() 中，当 doSomething 结束时指针对象已经被删除了，接下来（译注：*myTest = 12;）你会触发未定义行为（undefined behavior），甚至可能崩溃。 译注：原文 maybe even a crash!。实际触发 ub 时，崩溃可能是最好的结果了，很大程度免去了把 bug 带到线上的痛苦。 到 C++11，我们有了智能指针：unique_ptr、shared_ptr 和 weak_ptr。配合移动语义，C++ 终于可以正确支持唯一资源的转移了。而且新的智能指针可以被存储在标准容器内，auto_ptr 是不支持的。你应该用 unique_ptr 替换掉 auto_ptr，因为它是 auto_ptr 直接的、最好的对等物。 新的智能指针相比 auto_ptr 更强大更安全，所以从 C++11 起 auto_ptr 已经被标记为已弃用。编译器会报出一个 warning： warning: 'template&lt;class&gt; class std::auto_ptr' is deprecated 现在，如果你用一个合规的 C++17 编译器编译，它会报出一个 error。 这是启用 /std:c++latest 选项的 MSVC 2017 报出的 error： 1error C2039: 'auto_ptr': is not a member of 'std' 如果你需要从 auto_ptr 转换到 unique_ptr 的帮助，可以查阅 Clang Tidy，它提供了自动转换： Clang Tidy: modernize-replace-auto-ptr。 译注：Clang-Tidy 是一套基于 Clang 的静态检查工具，其中一项功能就是自动替换 auto_ptr。 扩展：本修改提案：N4190。","link":"/C++17%E8%AF%A6%E8%A7%A32.html"},{"title":"C++17 详解 1","text":"本文为 《C++17 in detail》 一书的中文渣中渣译文，不足之处还望指正。 第一部分——语言特性C++17 是对 C++ 的一次重大更新，引入了大量语言特性。大多数新增特性让 C++ 更简洁更直截了当。 在本部分你将学到： 被移除和被标记为已弃用的特性 C++ 是如何变得更精准的：比如，借助表达式求值顺序保证 模版相关新特性：比如 if constexpr、折叠表达式（fold expressions） 新的标准属性（attributes） 如何借助结构化绑定（structured binding）、内联变量（inline variables）、编译时 if 和类模版参数推导（template argument deduction for classes）写出更简洁更具表现力的代码 快速开始为了激发你对新标准更多的好奇心，下面展示几段新特性组合使用的代码示例。 如果觉得示例太复杂也不用担心，因为它们把太多新东西混在了一起。所有新特性都会在接下来的章节进行单独深入解释。 使用 Map123456789101112131415// Example: Part I/demo_map.cpp#include &lt;iostream&gt;#include &lt;map&gt;int main() { std::map&lt;std::string, int&gt; mapUserAge{ {\"Alex\", 45}, {\"John\", 25} }; std::map mapCopy{ mapUserAge }; if (auto [iter, wasAdded] = mapCopy.insert_or_assign(\"John\", 26); !wasAdded) std::cout &lt;&lt; iter-&gt;first &lt;&lt; \" reassigned...\\n\"; for (const auto &amp;[key, value] : mapCopy) std::cout &lt;&lt; key &lt;&lt; \", \" &lt;&lt; value &lt;&lt; '\\n';} 代码输出如下： 123John reassigned...Alex, 45John, 26 上边示例使用了如下特性： 第 8 行：类模板参数推导—— mapCopy 类型从 mapUserAge 类型推断得来。不需要显式声明为 std::map&lt;std::string, int&gt; mapCopy{...}。 第 10 行：新的 map 插入函数—— insert_or_assign。 第 10 行：结构化绑定——捕获 insert_or_assign 返回的 pair 到两个分开的变量。 第 10 行：带初始化的 if 语句—— iter、wasAdded 只在 if 语句代码块内可见。 第 13 行：基于范围的 for （range-based for）循环内的结构化绑定——可以用 key 和 value 代替 pair.first pair.second 进行迭代。 调试打印123456789101112131415161718192021222324252627// Example: Part I/demo_print.cpp#include &lt;iostream&gt;template&lt;typename T&gt; void linePrinter(const T &amp;x) { if constexpr (std::is_integral_v&lt;T&gt;) std::cout &lt;&lt; \"num: \" &lt;&lt; x &lt;&lt; '\\n'; else if constexpr (std::is_floating_point_v&lt;T&gt;) { const auto frac = x - static_cast&lt;long&gt;(x); std::cout &lt;&lt; \"flt: \" &lt;&lt; x &lt;&lt; \", frac \" &lt;&lt; frac &lt;&lt; '\\n'; } else if constexpr (std::is_pointer_v&lt;T&gt;) { std::cout &lt;&lt; \"ptr, \"; linePrinter(*x); } else std::cout &lt;&lt; x &lt;&lt; '\\n';}template &lt;typename ... Args&gt; void printWithInfo(Args ... args) { (linePrinter(args), ...); // 逗号运算符上的折叠表达式}int main() { int i = 10; float f = 2.56f; printWithInfo(&amp;i, &amp;f, 30);} 代码输出如下： 123ptr, num: 10ptr, flt: 2.56, frac 0.56num: 30 这里用到了以下特性： 第 5、7、11 行：if constexpr ——用于匹配模版参数，以使编译时丢弃代码。 第 5、7、11 行：类型萃取中 _v 样式的变量模版——不再需要写明 std::trait_name&lt;T&gt;::value。 第 20 行：printWithInfo 内的折叠表达式——此特性简化了可变参数模版。本例中我们对所有输入参数分别调用 linePrinter()。 正式开始吧！上边你看到的只是冰山一角，阅读接下来的章节你会看到更多：对当前语言的修复、阐明、移除（比如 auto_ptr），当然还有新增的一些东西：constexpr lambda，if constexpr，结构化绑定，template&lt;auto&gt;，内联变量，类模板参数推断以及更多。","link":"/C++17%E8%AF%A6%E8%A7%A31.html"},{"title":"C++17 详解 3","text":"本文为 《C++17 in detail》 一书的中文渣中渣译文，不足之处还望指正。 1.1.2 删除关键字 register关键字 register 在 2011 年（C++11）被弃用，自那以后 register 已经没有意义，现在它被删除。这个关键字仍然被保留，有可能会在未来某个版本重新设计意图（比如关键字 auto 就是复用而来，现在它是一个全新的、更强大的特性）。 如果你用 register 声明变量： 1register int a; 可能会产生如下 warning（GCC8.1）: 1warning: ISO C++17 does not allow 'register' storage class specifier 或是 Clang 的一个 error（Clang 7.0）： 1error: ISO C++17 does not allow 'register' storage class specifier 扩展：本修改提案：P0001R1 1.1.3 删除弃用的 operator++(bool)这个操作符已经被弃用很久了。标准委员会早在 1998 年（C++98）就建议不要使用，但直到现在他们才达成一致，把它从标准里删除。 译注：C++ 标准委员会出了名的松散、低效，学院气十足。此处即可见一斑，遑论如网络库、协程库等模块级的更新效率。 如果你尝试编译如下代码： 12bool b;b++; 会生成像下边 GCC（GCC 8.1）类似的 error： 1error: use of an operand of type 'bool' in 'operator++' is forbidden in C++17 扩展：本修改提案：P0002R1 1.1.4 删除弃用的异常规范C++17 里，异常规范是类型系统的一部分（在下一章“语言声明”有讨论）。但是标准还是保留了以前被弃用的异常规范，只是没有实际作用。 比如： 12345void fooThrowsInt(int a) throw(int) { printf_s(\"can throw ints\\n\"); if (a == 0) throw 1;} 请特别留意 throw(int)。 上边的代码在 C++11 已经被弃用，现在唯一可行的异常声明是 throw()，意思是这段代码绝不会抛出任何异常。C++11 起建议用 noexcept 代替。 比如 clang 4.0 里会生成如下 error： 12error: ISO C++1z does not allow dynamic exception specifications[-Wdynamic-exception-spec] note: use 'noexcept(false)' instead 扩展：本修改提案：P0003R5 1.1.5 其它被删除的特性这里列出其它被删除的比较小的内容： std::random_shuffle 此算法在 C++14 已被标记为弃用。原因是大多数的实现里都用到了 rand()，这个函数不够高效甚至容易出错（因为它用到了全局状态）。如果你需要同样的功能可以用： 12template&lt; class RandomIt, class URBG &gt;void shuffle( RandomIt first, RandomIt last, URBG&amp;&amp; g ); std::shuffle 接受一个随机数生成器作为第三个参数。更多参见 N4190。 删除旧的 functional 相关 bind1st() / bind2st() / mem_fun() 等这些函数，在 C++98 时代被引入，现在已经不需要了，你可以用 lambda。更重要的是，这些函数没有更新以处理完美转发、decltype 以及其它 C++11 以来的现代化技术。因此最好不要在现代 C++ 里用它们。更多参见 N4190。 删除三标符（trigraphs） 三标符是一种特殊的字符序列，在一些不支持 7-bit ASCII 码的系统（比如 ISO 646）上有用。比如 ??= 生成 #，??- 代表 ~（所有 C++ 源代码字符集都包含在 7-bit ASCII 码中）。在今天三标符已经很少用到，把它从翻译阶段移除，编译处理会更直截了当。参见 N4086。 译注：阅读本书之前，我完全不知道三标符的存在，只能勉强翻译。除上边草案外，还可以参考这里。","link":"/C++17%E8%AF%A6%E8%A7%A33.html"},{"title":"C++17 详解 4","text":"本文为 《C++17 in detail》 一书的中文渣中渣译文，不足之处还望指正。 1.2 修复（Fixes）修复的定义是有争论的。下边 3 条应该算是对在之前规则下缺失、不能正常工作的内容的修复。 1.2.1 直接列表初始化上 auto 的新规则C++11 起引入了一个奇怪的问题： 1auto x{ 1 }; 被推断为 std::initializer_list&lt;int&gt;。大多数情况下这个行为不是本意，你原本期望它像 int x{ 1 }; 一样工作。花括号初始化是现代 C++11 里首推的初始化样式，但是这种异常让这个特性变弱了。 在新标准里，我们可以修复它，以使它被推断为 int。 若要如此，我们需要理解两种初始化方式——拷贝（初始化）和直接（初始化）： 12345auto x = foo(); // 拷贝初始化auto x{foo()}; // 直接初始化, 初始化为 initializer_list (until C++17)int x = foo(); // 拷贝初始化int x{foo()}; // 直接初始化 C++17 为直接初始化引入了两条新规则： 对只有一个元素的花括号初始化列表，auto 推断会根据列表项推断； 对包含多于一个元素的花括号初始化列表，auto 推断是非良构的。 译注：非良构（ill-formed）代码即错误代码，会导致编译错误。 举几个例子： 12345auto x1 = { 1, 2 }; // decltype(x1) 值为 std::initializer_list&lt;int&gt;auto x2 = { 1, 2.0 }; // error: cannot deduce element typeauto x3{ 1, 2 }; // error: not a single elementauto x4 = { 3 }; // decltype(x4) 值为 std::initializer_list&lt;int&gt;auto x5{ 3 }; // decltype(x5) 值为 int 扩展： 本修改提案：N3922、N3681。编译器很早就修复了这个问题，GCC 5.0 （2015 年中）、Clang 3.8（2016 年初）、MSVC 2015（2015 年中）里此项改进就已可得，远早于 C++17 被批准。 1.2.2 不带消息体的 static_assert本特性新加了一个 static_assert 的重载版本，允许你只通过条件使用 static_assert 而不用传递消息参数。 它和其它的断言实现兼容，比如 BOOST_STATIC_ASSERT。有 boost 经验的程序员可以很容易切换到 C++17 的 static_assert。 12static_assert(std::is_arithmetic_v&lt;T&gt;, \"T must be arithmetic\");static_assert(std::is_arithmetic_v&lt;T&gt;); // C++17 起不再需要消息体 大多数情况下，条件本身就已足够表达意图，无需在消息字符串中再提及。 扩展：本修改提案：N3928。 1.2.3 range-based for 循环中使用类型不同的 begin 和 endC++11 引入了 range-based for 循环： 1234for (for-range-declaration : for-range-initializer){ statement;} 根据标准，这种循环表达式等同于如下代码： 123456789{ auto &amp;&amp; __range = for-range-initializer; for ( auto __begin = begin-expr, __end = end-expr; __begin != __end; ++__begin ) { for-range-declaration = *__begin; statement }} 如你所见，__begin 和 __end 类型相同。这样能很好地工作，但是伸缩性不够。比如你可能会想一直迭代到某个跟范围起点类型不同的哨兵。 C++17 里它被改为： 123456789{ auto &amp;&amp; __range = for-range-initializer; auto __begin = begin-expr; auto __end = end-expr; for ( ; __begin != __end; ++__begin ) { for-range-declaration = *__begin; statement }} __begin 和 __end 的类型不一样，只有比较操作符是必要的。这个改动对 for 的使用者没有任何可见的后果，但是对库的实现多了更多选择。比如，这个小改动允许 Range 技术规范（C++20 中的 Ranges）可以和 range for 循环一起工作。 扩展：本修改提案：P0184R0。 1.3 编译器支持略。 译注：懒得翻译了，到今天 C++17 已经被三大编译器厂商支持了 N 年了。","link":"/C++17%E8%AF%A6%E8%A7%A34.html"},{"title":"C++17 详解 5","text":"本文为 《C++17 in detail》 一书的中文渣中渣译文，不足之处还望指正。 2. 语言阐明学习并完全理解 C++ 是很有挑战的，许多地方都让程序员很疑惑。缺乏明确行为的一个原因，可能是赋予了编译器实现自由选择的权利。比如，允许更激进的优化，或者为了向后兼容（或者兼容 C）的需要。C++17 回顾了几个最出名的“黑洞”并把它们做了处理。 本章你将学到： 什么是求值顺序，求值顺序为什么会导致非预期结果。 复制消除（一种可选的优化手段，似乎在所有流行编译器上都已被实现。） 异常作为函数声明的一部分。 （过度）对齐数据的内存分配。 2.1 更严格的表达式求值顺序C++17 之前标准一直没有明确规定函数参数的求值顺序。呜呼哀哉。 比如，这也是为什么说在 C++14 里 make_unique 不只是语法糖的原因，它还保证了内存安全： 看下边这个例子： 1foo(make_unique&lt;T&gt;(), otherFunction()); 和显式 new 版本： 1foo(unique_ptr&lt;T&gt;(new T), otherFunction()); 上边代码在 C++14 里，我们知道 new T 被保证会在 unique_ptr 构造之前执行，不过也仅此而已。new T 可能会率先执行，然后是 otherFunction()，最后才是 unique_ptr 构造。 当 otherFunction() 抛出异常，new T 就会导致一次内存泄漏（智能指针对象还没有创建）。但如果你用的是 make_unique，它不可能导致内存泄漏，即使执行顺序未知。 C++17 解决了这个问题，现在属性的求值顺序是“实用的”、可预测的。 译注：原文为 “C++17 addresses this issue, and now the evaluation order of attributes is “practical” and predictable.” 2.1.1 示例在如下表达式里： 1f(a, b, c); a b c 的求值顺序依然不明确，但是任一参数都会在下一个参数执行前被全部计算。这点在如下的复杂表达式里至关重要： 1f(a(x), b, c(y)); 当编译器决定计算第一个参数 a(x) 时，同样需要在处理 b 或 c(y) 之前计算 x。 它同样修复了上边 make_unique 和 unique_ptr&lt;t&gt;(new T()) 的问题——因为函数参数必须在其它参数开始执行前被完全求值。 考虑下边的情况： 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;class Query{ public: Query&amp; addInt(int i) { std::cout &lt;&lt; \"addInt: \" &lt;&lt; i &lt;&lt; '\\n'; return *this; } Query&amp; addFloat(float f) { std::cout &lt;&lt; \"addFloat: \" &lt;&lt; f &lt;&lt; '\\n'; return *this; }};float computeFloat(){ std::cout &lt;&lt; \"computing float... \\n\"; return 10.1f;}float computeInt(){ std::cout &lt;&lt; \"computing int... \\n\"; return 8;}int main(){ Query q; q.addFloat(computeFloat()).addInt(computeInt());} C++14 里，你可能会期望 computeInt() 后于 addFloat() 执行。不幸的是它有可能不是这样执行。下边是 GCC 4.7.3 的输出： 1234computing int...computing float...addFloat: 10.1addInt: 8 函数调用链被明确规定为自左向右，但是内层的表达式的求值顺序是可能不一样的。更准确地来说： 表达式间的相互顺序是不确定的。 译注：原文为 “The expressions are indeterminately sequenced with respect to each other.”。 不过现在，在 C++17 里，包含嵌套表达式的函数调用链会如期运行，即自左向右： 在如下表达式里： 1a(expA).b(expB).c(expC) expA 会先于 b 执行。 用一个合规的 C++17 编译器编译上边的示例代码，会生成如下结果： 1234computing float...addFloat: 10.1computing int...addInt: 8 本次修改的另一个结果是，当进行操作符重载时，其执行顺序取决于原本内置操作符的关联性。 这也是为什么 std::cout &lt;&lt; a() &lt;&lt; b() &lt;&lt; c() 会按 a b c 顺序执行的原因。C++17 之前，它可以是任意执行顺序。 下边是标准描述的更多规则： 下列表达式均按先 a 后 b 的顺序计算： a.b a-&gt;b a-&gt;*b a(b1, b2, b3) // b1, b2, b3 - 任意顺序 b @= a // '@' means any operator a[b] a &lt;&lt; b a &gt;&gt; b 如果你不确定自己的代码如何被计算，你最好做一次简化，把代码拆成多个清晰的语句。你可以在《Core C++ Guidelines》里找到指导，比如ES.44¹ 和 ES.44²。 扩展：本修改提案：P0145R3。","link":"/C++17%E8%AF%A6%E8%A7%A35.html"},{"title":"C++17 详解 6","text":"本文为 《C++17 in detail》 一书的中文渣中渣译文，不足之处还望指正。 2.2 有保证的复制消除复制消除是一种流行的优化手段，它可以避免创建不必要的临时对象。 比如： 1234567891011121314151617181920// Chapter Clarification/copy_elision.cpp#include &lt;iostream&gt;struct Test{ Test() { std::cout &lt;&lt; \"Test::Test\\n\"; } Test(const Test&amp;) { std::cout &lt;&lt; \"Test(const Test&amp;)\\n\"; } Test(Test&amp;&amp;) { std::cout &lt;&lt; \"Test(Test&amp;&amp;)\\n\"; } ~Test() { std::cout &lt;&lt; \"~Test\\n\"; }};Test Create(){ return Test();}int main(){ auto n = Create();} 上边的调用中，为了存储 Create 的返回值，你可能会假设用到了一次临时的复制。C++14 里，大多数编译器都能注意到这个临时对象很容易被优化掉，n 可以从 Create() 中“直接”创建。你很可能会得到如下输出： 12Test::Test // 创建 n~Test // main 结束时销毁 n 这种复制消除的优化行为，依照其基本形式，被叫做返回值优化（Return Value Optimisation——RVO）。 作为实验，你可以在 GCC 里添加编译选项 -fno-elide-constructors 和 -std=c++14（或者其它更早的语言标准）。这时你会看到不同的输出结果： 1234567// compiled as &quot;g++ CopyElision.cpp -std=c++14 -fno-elide-constructors&quot;Test::TestTest(Test&amp;&amp;)~TestTest(Test&amp;&amp;)~Test~Test 这种情况下，编译器用了 2 次额外的复制把返回值传递到 n。 编译器甚至能更聪明，它可以在你返回一个命名对象的情况下进行消除——即所谓的命名返回值优化（Named Return Value Optimisation——NRVO）: 12345678Test Create(){ Test t; // 这里对 t 进行初始化 return t;}auto n = Create(); // 临时对象通常能被消除（译注：t 怎么就变成临时对象了呢？） 当前（译注：指 C++17 之前），标准允许以下情况下的消除： 临时对象被用于初始化另一个对象（包括函数返回的对象、通过 throw 表达式创建的异常对象）时 当一个即将超出范围的变量被返回或抛出时 异常被按值捕获时 但是，消除与否取决于编译器实现。现实是所有的构造函数（译注：即构造、拷贝构造、移动构造）都是必要的。有时候消除只会在 release 构建（优化过的）时发生，debug 构建（没有任何优化选项）不会消除任何东西。 C++17 里，对何时应进行消除有了清楚的规则，甚至构造函数也可能被完全省略。 这样有什么作用呢？ 允许返回不可移动/不可复制的对象——因为现在可以跳过拷贝/移动构造 提升代码可移植性——因为每一个合规的编译器都支持同样的规则 支持按值返回样式而不是使用输出参数 提升性能 下边是一个不可移动/不可复制的类型的例子： 1234567891011121314151617181920212223242526// Chapter Clarification/copy_elision_non_moveable.cpp#include &lt;array&gt;// based on P0135R0struct NonMoveable{ NonMoveable(int x) : v(x) { } NonMoveable(const NonMoveable&amp;) = delete; NonMoveable(NonMoveable&amp;&amp;) = delete; std::array&lt;int, 1024&gt; arr; int v;};NonMoveable make(int val){ if (val &gt; 0) return NonMoveable(val); return NonMoveable(-val);}int main(){ auto largeNonMoveableObj = make(90); // construct the object return largeNonMoveableObj.v;} C++14 里上边的代码会编译失败，因为它缺少拷贝、移动构造函数。但是在 C++17 里这些构造函数不再是必要的了——因为对象 largeNonMoveableObj 会被原地构造。 注意，你可以在函数内使用多个返回语句，复制消除一样可以生效。 此外，重要的是要记住，C++17 的复制消除只对临时对象起作用，对 NRVO 无效。 这种强制性的复制消除在标准里是怎么定义的呢？此功能基于值类别（Value Categories），请继续阅读下一节内容以理解其工作原理。 2.2.1 更新后的值类别C++98/03 里只有两种基本的表达式类别： lvalue rvalue 译注：即左值和右值。 C++11 起这种分类被扩展了（因为有了移动语义），现在有五种类别： lvalue glvalue（泛左值） xvalue（亡值、将亡值） rvalue prvalue（纯右值） 这里有个图表，可以更好地一览所有分类： 请记住，我们有三种核心类别（下边是口语化的“定义”）: lvalue ——有标识符的表达式，且可以取地址 xvalue ——“即将过期（eXpiring）的 lvalue”——可以移动、可以重复使用的对象，通常其生命周期马上结束 prvalue ——（pure rvalue）——没有名字、不能被取地址、可以移动的表达式 译注：C++17 中 prvalue 已经是不可移动了。 为了支持标准化的复制消除，提案作者建议简化 glvalue 和 prvalue 的定义： glvalue ——泛化的 lvalue —— glvalue 是其求值确定一个对象、位域或函数的位置的表达式 译注：原文：“glvalue - “generalised” lvalue - A glvalue is an expression whose evaluation computes thelocation of an object, bit-field, or function” prvalue ——纯 rvalue —— prvalue 是其求值初始化某个对象或位域，或计算某个运算符的操作数的值（依它所出现的上下文而定。译注：即虽然看起来同样的表达式，其类别也不同，比如 a[n]、a.m）的表达式 译注：原文：“prvalue - “pure” rvalue - A prvalue is an expression whose evaluation initialises an object,bit-field, or operand of an operator, as specified by the context in which it appears” 比如： 1234class X { int a; };X{10} // 表达式为 prvalueX x; // x 是 lvaluex.a // 表达式为 lvalue (location) 简言之：prvalue 执行初始化，glvalue 描述位置。 C++17 规定，当你从某个类或数组的 prvalue 对象进行初始化时，不需要创建临时对象。没有任何移动或拷贝牵涉其中（所以也就不需要必须有拷贝或移动构造函数了）；编译器可以安全地进行消除。 它会发生在如下情况： 从一个 prvalue 类别的对象初始化：Type t = T() 一个返回 prvalue 的函数的调用时——跟上边几个例子一样。 有几个例外情况，临时对象仍然是必需的： prvalue 被绑定到某个引用 在一个 prvalue 类别的类对象上执行成员访问 在一个 prvalue 类别的数组上执行下标操作 一个 prvalue 类别的数组被退化到指针 在一个 prvalue 类别的类对象上进行子类到基类的类型转换 prvalue 被用作舍弃的值表达式 扩展：本修改提案：P0135R0 （论证）和 P0135R1（措辞）。 译注： 翻译这节要了老命了！ 又一标准委员会过于学院气的铁证。为了支撑新的语法、新的特性，常常无端构造出一些形而上学的概念，从最初的左值、右值的推出即有此意。 到 C++11，已经彻底不说人话了，用词晦涩难懂，又无法通过 traits 验证结果（仅有 is_lvalue_reference、is_rvalue_reference 几个相关的 traits）。 C++17 更是变本加厉。可以看上述两个提案，简单地说，C++17 通过对五种类别的定义措辞做了所谓的“微调”（tweak），支撑了 Copy Elision 的标准化。这不像是语言设计，更像是某种不可言传、神乎其神的内功心法。 徒耗标准委员会的时间外，这种毫无表征的东西的存在，势必会导致语言理解的复杂度。我们常说要少写注释，让代码清晰到可以自我解释。语言的设计更应该如此，任何复杂特性的用法，都应该通过若干简单、清晰、可具体实施的语法组合实现，绝非通过这种架空的、哲学态的、虚无的定义实现。 回到本节内容本身。值类别前的一小节应该算是易懂的，不做赘述。值类别的更详细、更准确的表述和翻译，可以直接参考这里 和这里 。请恕我才疏学浅。","link":"/C++17%E8%AF%A6%E8%A7%A36.html"}],"tags":[{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"并行&#x2F;并发计算","slug":"并行-并发计算","link":"/tags/%E5%B9%B6%E8%A1%8C-%E5%B9%B6%E5%8F%91%E8%AE%A1%E7%AE%97/"},{"name":"AVX2","slug":"AVX2","link":"/tags/AVX2/"},{"name":"C++17","slug":"C-17","link":"/tags/C-17/"},{"name":"NEON","slug":"NEON","link":"/tags/NEON/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"Unity","slug":"Unity","link":"/tags/Unity/"},{"name":"VR","slug":"VR","link":"/tags/VR/"},{"name":"mono","slug":"mono","link":"/tags/mono/"},{"name":"Animator","slug":"Animator","link":"/tags/Animator/"},{"name":"Qt","slug":"Qt","link":"/tags/Qt/"},{"name":"VideoPlayer","slug":"VideoPlayer","link":"/tags/VideoPlayer/"},{"name":"XOPlayer","slug":"XOPlayer","link":"/tags/XOPlayer/"},{"name":"西安","slug":"西安","link":"/tags/%E8%A5%BF%E5%AE%89/"},{"name":"DeepSpeech","slug":"DeepSpeech","link":"/tags/DeepSpeech/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"Spectre","slug":"Spectre","link":"/tags/Spectre/"},{"name":"FFmpeg","slug":"FFmpeg","link":"/tags/FFmpeg/"},{"name":"NAS, Jellyfin","slug":"NAS-Jellyfin","link":"/tags/NAS-Jellyfin/"},{"name":"C++, C++17","slug":"C-C-17","link":"/tags/C-C-17/"}],"categories":[{"name":"杂记","slug":"杂记","link":"/categories/%E6%9D%82%E8%AE%B0/"},{"name":"并发计算","slug":"并发计算","link":"/categories/%E5%B9%B6%E5%8F%91%E8%AE%A1%E7%AE%97/"},{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"FFmpeg","slug":"FFmpeg","link":"/categories/FFmpeg/"},{"name":"C++","slug":"并发计算/C","link":"/categories/%E5%B9%B6%E5%8F%91%E8%AE%A1%E7%AE%97/C/"},{"name":"StreamingCore","slug":"StreamingCore","link":"/categories/StreamingCore/"},{"name":"使用 Unity 开发 Android VR 应用","slug":"使用-Unity-开发-Android-VR-应用","link":"/categories/%E4%BD%BF%E7%94%A8-Unity-%E5%BC%80%E5%8F%91-Android-VR-%E5%BA%94%E7%94%A8/"},{"name":"Unity","slug":"Unity","link":"/categories/Unity/"},{"name":"Qt","slug":"Qt","link":"/categories/Qt/"},{"name":"并发计算","slug":"C/并发计算","link":"/categories/C/%E5%B9%B6%E5%8F%91%E8%AE%A1%E7%AE%97/"},{"name":"FFmpeg 常用命令","slug":"FFmpeg-常用命令","link":"/categories/FFmpeg-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"name":"智能家居","slug":"智能家居","link":"/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Sanic","slug":"Python/Sanic","link":"/categories/Python/Sanic/"},{"name":"C++17","slug":"C/C-17","link":"/categories/C/C-17/"},{"name":"C++17详解","slug":"C/C-17/C-17详解","link":"/categories/C/C-17/C-17%E8%AF%A6%E8%A7%A3/"}]}